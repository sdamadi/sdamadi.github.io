<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Stochastic intuition behind Principle Component Analysis (PCA) in stochastic setting - Saeed Damadi</title><meta name="robots" content="index, follow"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://sdamadi.github.io/first-post.html"><link rel="amphtml" href="https://sdamadi.github.io/amp/first-post.html"><link type="application/atom+xml" rel="alternate" href="https://sdamadi.github.io/feed.xml"><meta property="og:title" content="Stochastic intuition behind Principle Component Analysis (PCA) in stochastic setting"><meta property="og:image" content="https://sdamadi.github.io/media/posts/1/javier-graterol-31710-unsplash.jpg"><meta property="og:site_name" content="My blog - Saeed Damadi"><meta property="og:description" content="Principle Component Analysis (PCA) in stochastic setting Why PCA is important is because It is used for obtaining a lower dimensional representation of a high dimensional data that still captures as much as possible of the original signal. Likewise, the picture, the reflection on the water is 2D&hellip;"><meta property="og:url" content="https://sdamadi.github.io/first-post.html"><meta property="og:type" content="article"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin><link href="https://fonts.googleapis.com/css?family=Roboto:400,700|Roboto+Condensed:400,700&amp;subset=latin-ext" rel="stylesheet"><link rel="stylesheet" href="https://sdamadi.github.io/assets/css/style.css?v=aa41620c4f80dc7a8874270e3bdddd5b"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://sdamadi.github.io/first-post.html"},"headline":"Stochastic intuition behind Principle Component Analysis (PCA) in stochastic setting","datePublished":"2018-12-30T23:06","dateModified":"2019-12-07T22:12","image":{"@type":"ImageObject","url":"https://sdamadi.github.io/media/posts/1/javier-graterol-31710-unsplash.jpg","height":3378,"width":4875},"description":"Principle Component Analysis (PCA) in stochastic setting Why PCA is important is because It is used for obtaining a lower dimensional representation of a high dimensional data that still captures as much as possible of the original signal. Likewise, the picture, the reflection on the water is 2D&hellip;","author":{"@type":"Person","name":"Saeed"},"publisher":{"@type":"Organization","name":"Saeed"}}</script><script async src="https://sdamadi.github.io/assets/js/lazysizes.min.js?v=dc4b666bb3324aea4ead22e26059c761"></script><style>.top__search [type=search] {
						background-image: url(https://sdamadi.github.io/assets/svg/search.svg);
				    }</style><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script></head><body><div class="container"><header class="js-top is-sticky"><a class="logo" href="https://sdamadi.github.io">Saeed Damadi</a><div class="top"><nav class="navbar js-navbar"><button class="navbar__toggle js-navbar__toggle">Menu</button><ul class="navbar__menu"></ul></nav><div class="top__search"><form action="https://sdamadi.github.io/search.html" class="search"><input type="search" name="q" placeholder="search..."></form></div></div></header><main><article class="post"><div class="hero"><figure class="hero__wrap"><img src="https://sdamadi.github.io/media/posts/1/javier-graterol-31710-unsplash.jpg" sizes="(max-width: 1600px) 100vw, 1600px" srcset="https://sdamadi.github.io/media/posts/1/responsive/javier-graterol-31710-unsplash-xs.jpg 300w, https://sdamadi.github.io/media/posts/1/responsive/javier-graterol-31710-unsplash-sm.jpg 480w, https://sdamadi.github.io/media/posts/1/responsive/javier-graterol-31710-unsplash-md.jpg 768w, https://sdamadi.github.io/media/posts/1/responsive/javier-graterol-31710-unsplash-lg.jpg 1024w, https://sdamadi.github.io/media/posts/1/responsive/javier-graterol-31710-unsplash-xl.jpg 1360w, https://sdamadi.github.io/media/posts/1/responsive/javier-graterol-31710-unsplash-2xl.jpg 1600w" alt=""><figcaption>Photo by Javier Graterol</figcaption></figure><header class="hero__text"><h1>Stochastic intuition behind Principle Component Analysis (PCA) in stochastic setting</h1><p class="post__meta"><time datetime="2018-12-30T23:06">December 30, 2018 </time>By <a href="https://sdamadi.github.io/authors/saeed/" rel="author" title="Saeed">Saeed</a></p></header></div><div class="post__entry"><p style="margin: 0in; margin-bottom: .0001pt;"><strong><span style="font-size: 12.5pt; font-family: 'Arial',sans-serif; color: #2a2e30;">Principle Component Analysis (PCA) in stochastic setting</span></strong></p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;"><span style="font-size: 12.5pt; font-family: 'Arial',sans-serif; color: #2a2e30;">Why PCA is important is because It is used for obtaining a lower dimensional representation of a high dimensional data that still captures as much as possible of the original signal. Likewise, the picture, the reflection on the water is 2D representation of 3D city which need less space!</span></p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;"><span style="box-sizing: border-box;"><span style="font-size: 12.5pt; font-family: 'Arial',sans-serif; color: #2a2e30;">To understand Principle Component Analysis (PCA) in stochastic setting assume $x \in \mathbb{R}^n$ be a random vector. Also, assume $x$ is distributed according to a distribution $D$ which is unknown.</span></span></p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;"><span style="font-size: 12.5pt; font-family: 'Arial',sans-serif; color: #2a2e30;">Knowing the fact that $x$ lives in $\mathbb{R}^n$, immediately brings a question whether $x$ is bounded or not. When $x$ is not bounded we can say nothing about how big it is but when $x$ is bounded we know it can be contained in a ball with radius $R$ where $\mathbb{E}[\|x\|_2^2] \leq R$.</span></p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;"><span style="font-size: 12.5pt; font-family: 'Arial',sans-serif; color: #2a2e30;"><strong style="box-sizing: border-box;">Note</strong>: Talking about boundedness requires a measure which is $\ell_2$-norm in this case (but it could be any norm). In addition, the reason behind the expected value is that we are dealing with a random vector that takes on any value in $\mathbb{R}^n$, but cannot talk about the exact maximum of $x$. Hence, we are interested in the maximum it can achieve on the average, that is why we have expectation.</span></p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">Now the radius of that ball is important because it gives us the expected boundary of the space that $x$ lives in it:</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\max \,\,\,\, \mathbb{E}[\|x\|_2^2]$$</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\text{s.t.} \,\,\,\, x \sim D$$</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">Since the quantity inside the expectation is a scalar, we know that the trace of a scalar is the same as itself so</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\max \,\,\,\, \mathbb{E}[tr(x^Tx)]$$</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\text{s.t.} \,\,\,\, x \sim D$$</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">where $\|x\|_2^2=x^Tx$.</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">Writing the objective as $\mathbb{E}[tr(x^Tx)]]=\mathbb{E}[tr(x^TxI_d)]]$ where $I_d \in \mathbb{R}^{d \times d}$ is the identity matrix, and using <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)">cyclic permutation</a> of trace operator we have</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\max\,\,\,\, \mathbb{E}[tr(xIx^T)] \tag{1}$$</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\text{s.t.} \,\,\,\, x \sim D$$</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">We know that $I_d$ can be written as $I_d=UU^T$ where $U \in \mathbb{R}^{d \times d}$ is a matrix whose columns are $d$ permutations of Euclidean basis set $\{e_1,e_2, \cdots, e_d\}$. For example $U$ might be as follows $$U=\begin{bmatrix}e_d &amp; e_{d-1} &amp; \cdots &amp; e_1\end{bmatrix}$$ </p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">where $e_i \in \mathbb{R}^{d}$ has $1$ for $i$-th componenet and the rest are zero. So</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$I_d=UU^T=\begin{bmatrix}e_d &amp; e_{d-1} &amp; \cdots &amp; e_1\end{bmatrix}\begin{bmatrix}e_k^T \\ e_{d-1}^T \\ \cdots \\ e_1^T\end{bmatrix}=e_de_d^T+e_{d-1}e_{d-1}^T+ \cdots + e_1e_1^T$$</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">where each $E_i=e_ie_i^T$ is a $d \times d$ matrix whose $E_{ii}$ comonent is $1$ and the rest is zero. Any permutation of the set $\{e_1,e_2, \cdots, e_d\}$ adds up to $I_d$. Totally, we have $d!$ permutations of Euclidean basis set to build $I_d$.</p><p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">We know that $I_d$ is a projection matrix from $\mathbb{R}^{d}$ to $\mathbb{R}^{d}$. However, we are looking for the projection onto a subspace with lower dimentional $k$. Projection from $\mathbb{R}^{d}$ to $\mathbb{R}^{k}$ is done using $P_k=U_k^TU_k$ where $U_k \in \mathbb{R}^{d \times k}$ is a matrix whose columns are $k$ permutations of Euclidean basis set $\{e_1,e_2, \cdots, e_d\}$. Therefore, $P_k$ has $k$ ones on the diagonal and the rest is zero. Let $I_k^{\perp}=I_d-I_k$. Then (1) becomes</p><p class="post__last-updated">This article was updated on December 7, 2019</p></div><aside class="post__share"></aside><footer class="post__footer"><div class="post__bio"><h3><a href="https://sdamadi.github.io/authors/saeed/" class="inverse" title="Saeed">Saeed</a></h3></div><nav class="post__nav"><div class="post__nav__next">Next Post<h5><a href="https://sdamadi.github.io/operation-complexity-of-familiar-matrix-multiplication.html" class="inverse" rel="next">Operation complexity of familiar matrix multiplications</a></h5></div></nav></footer></article></main><footer class="footer"><div class="footer__copyright">Powered by Publii</div></footer></div><script defer="defer" src="https://sdamadi.github.io/assets/js/jquery-3.2.1.slim.min.js?v=f307ecc7f6353949f03c06ffa70652a2"></script><script defer="defer" src="https://sdamadi.github.io/assets/js/scripts.min.js?v=f37607af05f83050cdcc37ea18bb5ee1"></script></body></html>