<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Saeed Damadi</title>
    <link href="https://sdamadi.github.io/feed.xml" rel="self" />
    <link href="https://sdamadi.github.io" />
    <updated>2019-12-23T21:25:50-05:00</updated>
    <author>
        <name>Saeed</name>
    </author>
    <id>https://sdamadi.github.io</id>

    <entry>
        <title>What is Jacobian matrix and why do we need it?</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html"/>
        <id>https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html</id>

        <updated>2019-12-23T21:25:25-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/4/" alt="" />
                    Derivative of univariate functionTo understand what is Jacobian, we need to revisit the derivative of a univariate function wherein $f$ maps the real line into the real line, that is, $f: \mathbb{R} \rightarrow \mathbb{R} $. The derivative of $f$ denoted by $f'$ measures the sensitivity to change&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/4/" alt="" />
                <h3 id="mcetoc_1dsqkbs450">Derivative of univariate function</h3>
<p>To understand what is Jacobian, we need to revisit the derivative of a univariate function wherein $f$ maps the real line into the real line, that is, $f: \mathbb{R} \rightarrow \mathbb{R} $. The derivative of $f$ denoted by $f'$ measures the sensitivity to change of the function value (output value, $f(x)$) with respect to a change in its argument (input value, $x$). Now the question is how we can measure the change of a function whose inputs is a vector in $\mathbb{R}^n$. From this point on we will focus on the change in a value of a function and try to associate it to the derivative of a univariate function.</p>
<p><strong>Note</strong>: $f'$ is also denoted by $\frac{df}{dx}$ which is read as the derivative of $f$ with respect to $x$. Here we have used $d$ since there is no other variables that derivative is being taken with respect to them.</p>
<p><strong>Remark: </strong>If $x$ becomes a number in real line instead of a vector, the gradient becomes the derivative which was expected.</p>
<h3 id="mcetoc_1dsqkdumb1">The gradient of a real-valued function</h3>
<p>When $f$ takes on a vector $x \in \mathbb{R}^n$ where $x = [x_1, x_2, \cdots,x_n]^{\top}$ and maps it to a number on the real line, we write $f : \mathbb{R}^n \rightarrow \mathbb{R}$. Since we have the ability to play with each coordinate of $x$ to change the value of the function, the concept of gradient comes into the picture. The gradient is a vector with the same length of $x$ and each coordinate shows the change in the value of $f$ when the corresponding coordinate changes. Therefore, we write</p>
<p> \begin{equation}<br>\nabla f = [\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}]^{\top}<br>\end{equation}</p>
<p>Here we use $\nabla$ (nabla) symbol to suppress the right hand side when we want to address all the changes along $n$ directions. Also, $\partial$ is used to clarify that other than (let's say $x_1$), there are other variables whose change can affect the value of $f$.</p>
<h3 id="mcetoc_1dsqmf85h2">Jacobian of a vector-valued function</h3>
<p>Now let's get back to the question that I asked in the page title. You might say, "well, Jacobian matrix should be something that somehow connects to the change of a vector-valued function which takes a vector as its input". Yes correct! But how we do we write it?</p>
<p>First notice that we have $f$ where $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, that is:</p>
<p>$$<br> f(x)=f(x_1, x_2, \cdots,x_n)= \begin{bmatrix}<br> f_1(x_1, x_2, \cdots,x_n)\\ f_2(x_1, x_2, \cdots,x_n)\\ \vdots \\ f_n(x_1, x_2, \cdots,x_n)<br> \end{bmatrix}<br>$$</p>
<p>As you can see the value of each function $f_i$ where $i= 1, 2, \cdots, m$ can vary when one of $x_j$'s changes. This is where Jacobian matrix comes into the picture. It is defined as the following:</p>
<p>$$<br>J_f(x)= \begin{bmatrix}<br>\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2}&amp; \cdots&amp; \frac{\partial f_1}{\partial x_n}\\<br>\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2}&amp; \cdots&amp; \frac{\partial f_2}{\partial x_n}\\<br> \vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>\frac{\partial f_m}{\partial x_1} &amp; \frac{\partial f_m}{\partial x_2}&amp; \cdots&amp; \frac{\partial f_m}{\partial x_n}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\nabla^{\top} f_1\\<br>\nabla^{\top} f_2\\<br>\vdots\\<br>\nabla^{\top} f_n\\<br>\end{bmatrix}<br>$$</p>
<p>By looking at $i$-th row of the Jacobian matrix, we see that it is transpose of the gradient of $f_i$, i.e., $(\nabla f_i)^{\top}$ which I denoted by $\nabla^{\top} f_i$ . </p>
<p><strong>Remark 1</strong>: If $f$ becomes a scalar-valued function and $x$ becomes a number in real line instead of a vector, the Jacobian matrix becomes the derivative!</p>
<p><strong>Remark 2</strong>: Jacobian matrix is the matrix of all first-order partial derivatives of the function.</p>
<h3 id="mcetoc_1dsqpjd0t0">Why do we need Jacobian matrix?</h3>
<p>Jacobian of a composition function is the product of the Jacobian matrices of each functions. Suppose we have the following fucntion:</p>
<p>$$<br>f (x)= f_1(f_2(\cdots( f_n(x) ) ) = f_1 \circ f_2 \circ \cdots\circ f_n(x)<br>$$</p>
<p>The Jacobian matrix of $f$ with respect to $x$ is:</p>
<p>$$<br>J_f = J_{f_1} \cdot J_{f_2} \cdots J_{f_n}(x)<br>$$</p>
<p>However, there is a big caveat here; in order to find Jacobian matrix of $f$, we need to find all other Jacobian matrices. We know Jacobian matrix is all first-order partial derivatives of the vector-valued function with respect to its variable. But what is the variable of, let's say $f_1$? The variable of $f_1$ is the f but $f_1$, i.e., $f_2(f_3(\cdots( f_n(x) ) ) = f_2 \circ f_3 \circ \cdots\circ f_n(x)$. </p>
<p>In the </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Jacobian of a composition function: an example</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/jacobian-of-a-composition-function.html"/>
        <id>https://sdamadi.github.io/jacobian-of-a-composition-function.html</id>

        <updated>2019-12-23T18:51:01-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/3/" alt="" />
                    In this post we are going to see an example in which Jacobian operator is being applied on a composite function: Finite dimension is $\sqrt{x}$ What is Jacobian and why do we need it?
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/3/" alt="" />
                <p>In this post we are going to see an example in which Jacobian operator is being applied on a composite function:</p>
<p> </p>
<p><a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative#Finite_dimensions">Finite dimension</a> is $\sqrt{x}$</p>
<p> </p>
<h1 id="mcetoc_1dsqin8so0">What is Jacobian and why do we need it?</h1>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Operation complexity of familiar matrix multiplications</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/operation-complexity-of-familiar-matrix-multiplication.html"/>
        <id>https://sdamadi.github.io/operation-complexity-of-familiar-matrix-multiplication.html</id>

        <updated>2019-12-07T21:53:55-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/2/" alt="" />
                    Let $x \in \mathbb{R}^n$. 1- Why operations required for computing $\frac{1}{T}\sum_{t=1}^{T}x_tx_t^T$ is $O(d^2)$? 2- What is the upperbound for operation $xx^TU$ where $U \in \mathbb{R}^{n \times k}$
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/2/" alt="" />
                <p>Let $x \in \mathbb{R}^n$.</p>
<p>1- Why operations required for computing $\frac{1}{T}\sum_{t=1}^{T}x_tx_t^T$ is $O(d^2)$?</p>
<p>2- What is the upperbound for operation $xx^TU$ where $U \in \mathbb{R}^{n \times k}$</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Stochastic intuition behind Principle Component Analysis (PCA) in stochastic setting</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/first-post.html"/>
        <id>https://sdamadi.github.io/first-post.html</id>

        <updated>2019-12-07T22:14:01-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/1/javier-graterol-31710-unsplash.jpg" alt="" />
                    Principle Component Analysis (PCA) in stochastic setting Why PCA is important is because It is used for obtaining a lower dimensional representation of a high dimensional data that still captures as much as possible of the original signal. Likewise, the picture, the reflection on the water is 2D&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/1/javier-graterol-31710-unsplash.jpg" alt="" />
                <p style="margin: 0in; margin-bottom: .0001pt;"><strong><span style="font-size: 12.5pt; font-family: 'Arial',sans-serif; color: #2a2e30;">Principle Component Analysis (PCA) in stochastic setting</span></strong></p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;"><span style="font-size: 12.5pt; font-family: 'Arial',sans-serif; color: #2a2e30;">Why PCA is important is because It is used for obtaining a lower dimensional representation of a high dimensional data that still captures as much as possible of the original signal. Likewise, the picture, the reflection on the water is 2D representation of 3D city which need less space!</span></p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;"><span style="box-sizing: border-box;"><span style="font-size: 12.5pt; font-family: 'Arial',sans-serif; color: #2a2e30;">To understand Principle Component Analysis (PCA) in stochastic setting assume $x \in \mathbb{R}^n$ be a random vector. Also, assume $x$ is distributed according to a distribution $D$ which is unknown.</span></span></p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;"><span style="font-size: 12.5pt; font-family: 'Arial',sans-serif; color: #2a2e30;">Knowing the fact that $x$ lives in $\mathbb{R}^n$, immediately brings a question whether $x$ is bounded or not. When $x$ is not bounded we can say nothing about how big it is but when $x$ is bounded we know it can be contained in a ball with radius $R$ where $\mathbb{E}[\|x\|_2^2] \leq R$.</span></p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;"><span style="font-size: 12.5pt; font-family: 'Arial',sans-serif; color: #2a2e30;"><strong style="box-sizing: border-box;">Note</strong>: Talking about boundedness requires a measure which is $\ell_2$-norm in this case (but it could be any norm). In addition, the reason behind the expected value is that we are dealing with a random vector that takes on any value in $\mathbb{R}^n$, but cannot talk about the exact maximum of $x$. Hence, we are interested in the maximum it can achieve on the average, that is why we have expectation.</span></p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">Now the radius of that ball is important because it gives us the expected boundary of the space that $x$ lives in it:</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\max \,\,\,\, \mathbb{E}[\|x\|_2^2]$$</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\text{s.t.} \,\,\,\, x \sim D$$</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">Since the quantity inside the expectation is a scalar, we know that the trace of a scalar is the same as itself so</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\max \,\,\,\, \mathbb{E}[tr(x^Tx)]$$</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\text{s.t.} \,\,\,\, x \sim D$$</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">where $\|x\|_2^2=x^Tx$.</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">Writing the objective as $\mathbb{E}[tr(x^Tx)]]=\mathbb{E}[tr(x^TxI_d)]]$ where $I_d \in \mathbb{R}^{d \times d}$ is the identity matrix, and using <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)">cyclic permutation</a> of trace operator we have</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\max\,\,\,\, \mathbb{E}[tr(xIx^T)] \tag{1}$$</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$\text{s.t.} \,\,\,\, x \sim D$$</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">We know that $I_d$ can be written as $I_d=UU^T$ where $U \in \mathbb{R}^{d \times d}$ is a matrix whose columns are $d$ permutations of Euclidean basis set $\{e_1,e_2, \cdots, e_d\}$. For example $U$ might be as follows $$U=\begin{bmatrix}e_d &amp; e_{d-1} &amp; \cdots &amp; e_1\end{bmatrix}$$ </p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">where $e_i \in \mathbb{R}^{d}$ has $1$ for $i$-th componenet and the rest are zero. So</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">$$I_d=UU^T=\begin{bmatrix}e_d &amp; e_{d-1} &amp; \cdots &amp; e_1\end{bmatrix}\begin{bmatrix}e_k^T \\ e_{d-1}^T \\ \cdots \\ e_1^T\end{bmatrix}=e_de_d^T+e_{d-1}e_{d-1}^T+ \cdots + e_1e_1^T$$</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">where each $E_i=e_ie_i^T$ is a $d \times d$ matrix whose $E_{ii}$ comonent is $1$ and the rest is zero. Any permutation of the set $\{e_1,e_2, \cdots, e_d\}$ adds up to $I_d$. Totally, we have $d!$ permutations of Euclidean basis set to build $I_d$.</p>
<p style="box-sizing: border-box; margin: 1.6rem 0px 0px; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; text-align: start; widows: 2; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; word-spacing: 0px;">We know that $I_d$ is a projection matrix from $\mathbb{R}^{d}$ to $\mathbb{R}^{d}$. However, we are looking for the projection onto a subspace with lower dimentional $k$. Projection from $\mathbb{R}^{d}$ to $\mathbb{R}^{k}$ is done using $P_k=U_k^TU_k$ where $U_k \in \mathbb{R}^{d \times k}$ is a matrix whose columns are $k$ permutations of Euclidean basis set $\{e_1,e_2, \cdots, e_d\}$. Therefore, $P_k$ has $k$ ones on the diagonal and the rest is zero. Let $I_k^{\perp}=I_d-I_k$. Then (1) becomes</p>
            ]]>
        </content>
    </entry>
</feed>
