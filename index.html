<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>My blog - Saeed Damadi</title><meta name="robots" content="index, follow"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://sdamadi.github.io/"><link rel="amphtml" href="https://sdamadi.github.io/amp/"><link type="application/atom+xml" rel="alternate" href="https://sdamadi.github.io/feed.xml"><meta property="og:title" content="My blog - Saeed Damadi"><meta property="og:site_name" content="My blog - Saeed Damadi"><meta property="og:description" content=""><meta property="og:url" content="https://sdamadi.github.io/"><meta property="og:type" content="website"><link rel="next" href="https://sdamadi.github.io/page/2/"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin><link href="https://fonts.googleapis.com/css?family=Roboto:400,700|Roboto+Condensed:400,700&amp;subset=latin-ext" rel="stylesheet"><link rel="stylesheet" href="https://sdamadi.github.io/assets/css/style.css?v=aa41620c4f80dc7a8874270e3bdddd5b"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Organization","name":"Saeed Damadi","url":"https://sdamadi.github.io"}</script><script async src="https://sdamadi.github.io/assets/js/lazysizes.min.js?v=dc4b666bb3324aea4ead22e26059c761"></script><style>.top__search [type=search] {
						background-image: url(https://sdamadi.github.io/assets/svg/search.svg);
				    }</style><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script></head><body><div class="container"><header class="js-top is-sticky"><a class="logo" href="https://sdamadi.github.io">Saeed Damadi</a><div class="top"><nav class="navbar js-navbar"><button class="navbar__toggle js-navbar__toggle">Menu</button><ul class="navbar__menu"></ul></nav><div class="top__search"><form action="https://sdamadi.github.io/search.html" class="search"><input type="search" name="q" placeholder="search..."></form></div></div></header><main><section class="hero"><header class="hero__text"><h1>Simple - a blog style Publii theme</h1><p>Create a unique and beautiful blog quickly and easily.</p></header></section><div class="category"><article><header><p class="category__post-meta"><a href="https://sdamadi.github.io/authors/saeed/" class="inverse" title="Saeed">Saeed</a>, <time datetime="2019-12-26T18:34">December 26, 2019</time></p><h2><a href="https://sdamadi.github.io/stochastic-gradient-descent-versus-batch-gradient-descent.html" class="inverse">Stochastic gradient descent versus batch gradient descent</a></h2></header><p>The applicability of batch or stochastic gradient descent really depends on the error manifold expected. Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum&hellip;</p><a href="https://sdamadi.github.io/stochastic-gradient-descent-versus-batch-gradient-descent.html" class="u-readmore">Read more</a></article><article><header><p class="category__post-meta"><a href="https://sdamadi.github.io/authors/saeed/" class="inverse" title="Saeed">Saeed</a>, <time datetime="2019-12-25T20:12">December 25, 2019</time></p><h2><a href="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html" class="inverse">Creating nonlinear neural network and finding the Jacobian matrix of its funciton</a></h2></header><p>The next step towards finding loss function of a neural network is to extend the results we found here to add a bias to the linear function, i.e., creating $W^{\top}x+b$ where $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, and $W \in \mathbb{R}^{n \times m}$. This can be done easily because&hellip;</p><a href="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html" class="u-readmore">Read more</a></article><article><header><p class="category__post-meta"><a href="https://sdamadi.github.io/authors/saeed/" class="inverse" title="Saeed">Saeed</a>, <time datetime="2019-12-25T17:57">December 25, 2019</time></p><h2><a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html" class="inverse">Linear neural network to creat $f(x)&#x3D; W^{\top}x$</a></h2></header><p>Goal:In this post we are going to go through the very first step that help us to suppress algebraic notations for representing the function calculating by a fully connected neural network. Following this step helps you to master shorthand notations to express operations happening in&hellip;</p><a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html" class="u-readmore">Read more</a></article><article><header><p class="category__post-meta"><a href="https://sdamadi.github.io/authors/saeed/" class="inverse" title="Saeed">Saeed</a>, <time datetime="2019-12-25T14:57">December 25, 2019</time></p><h2><a href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html" class="inverse">Compute the gradient of a neural network</a></h2></header><p></p><a href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html" class="u-readmore">Read more</a></article><nav class="pagination"><a href="https://sdamadi.github.io/page/2/" class="btn" title="Previous">Previous</a></nav></div></main><footer class="footer"><div class="footer__copyright">Powered by Publii</div></footer></div><script defer="defer" src="https://sdamadi.github.io/assets/js/jquery-3.2.1.slim.min.js?v=f307ecc7f6353949f03c06ffa70652a2"></script><script defer="defer" src="https://sdamadi.github.io/assets/js/scripts.min.js?v=f37607af05f83050cdcc37ea18bb5ee1"></script></body></html>