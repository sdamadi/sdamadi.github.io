<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Saeed Damadi</title>
    <link href="https://sdamadi.github.io/feed.xml" rel="self" />
    <link href="https://sdamadi.github.io" />
    <updated>2020-03-01T22:10:45-05:00</updated>
    <author>
        <name>Saeed</name>
    </author>
    <id>https://sdamadi.github.io</id>

    <entry>
        <title>Stochastic gradient descent versus batch gradient descent</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/stochastic-gradient-descent-versus-batch-gradient-descent.html"/>
        <id>https://sdamadi.github.io/stochastic-gradient-descent-versus-batch-gradient-descent.html</id>

        <updated>2019-12-26T18:34:32-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/9/" alt="" />
                    The applicability of batch or stochastic gradient descent really depends on the error manifold expected. Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/9/" alt="" />
                <p>The applicability of batch or stochastic gradient descent really depends on the error manifold expected.</p>
<p>Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.</p>
<p>Stochastic gradient descent (SGD) computes the gradient using a single sample. Most applications of SGD actually use a minibatch of several samples, for reasons that will be explained a bit later. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal. Single samples are really noisy, while minibatches tend to average a little of the noise out. Thus, the amount of jerk is reduced when using minibatches. A good balance is struck when the minibatch size is small enough to avoid some of the poor local minima, but large enough that it doesn't avoid the global minima or better-performing local minima. (Incidently, this assumes that the best minima have a larger and deeper basin of attraction, and are therefore easier to fall into.)</p>
<p>One benefit of SGD is that it's computationally a whole lot faster. Large datasets often can't be held in RAM, which makes vectorization much less efficient. Rather, each sample or batch of samples must be loaded, worked with, the results stored, and so on. Minibatch SGD, on the other hand, is usually intentionally made small enough to be computationally tractable.</p>
<p>Usually, this computational advantage is leveraged by performing many more iterations of SGD, making many more steps than conventional batch gradient descent. This usually results in a model that is very close to that which would be found via batch gradient descent, or better.</p>
<p>The way I like to think of how SGD works is to imagine that I have one point that represents my input distribution. My model is attempting to learn that input distribution. Surrounding the input distribution is a shaded area that represents the input distributions of all of the possible minibatches I could sample. It's usually a fair assumption that the minibatch input distributions are close in proximity to the true input distribution. Batch gradient descent, at all steps, takes the steepest route to reach the true input distribution. SGD, on the other hand, chooses a random point within the shaded area, and takes the steepest route towards this point. At each iteration, though, it chooses a new point. The average of all of these steps will approximate the true input distribution, usually quite well.</p>
<p>In practice, nobody uses Batch Gradient Descent. It's simply too computationally expensive for not that much of a gain. (The gain being that you're actually stepping down the "true" gradient.) When you have a highly non-convex loss function you just need to step in mostly the right direction and you'll eventually converge on a local minimum. Thus, minibatch SGD.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Creating nonlinear neural network and finding the Jacobian matrix of its funciton</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"/>
        <id>https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html</id>

        <updated>2019-12-28T20:28:49-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/8/" alt="" />
                    The next step towards finding loss function of a neural network is to extend the results we found here to add a bias to the linear function, i.e., creating $W^{\top}x+b$ where $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, and $W \in \mathbb{R}^{n \times m}$. This can be done easily because&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/8/" alt="" />
                <p>The next step towards finding loss function of a neural network is to extend the results we found <a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html">here</a> to add a bias to the linear function, i.e., creating $W^{\top}x+b$ where $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, and $W \in \mathbb{R}^{n \times m}$. This can be done easily because we already have the part of a network that generates $W^{\top}x$. It suffice to add a constant vector to the previous result. Therefore, we have the follwing for the case where $n = 2$ and $m = 3$</p>
<figure class="post__image post__image--center" ><img src="https://sdamadi.github.io/media/posts/8/Neural-Network-with-bias.png" alt="" width="300" height="628">
<figcaption >Adding bias to the neural network</figcaption>
</figure>
<p>Now to create a nonlinear function $F(x)=f( W^{\top}x+b)$, we need to pass in the above quantity to the function $f$, which produces $F(x)$.</p>
<h3 id="mcetoc_1dt0dnt1e0"> Jacobian matrix of $F(x)=f( W^{\top}x+b)$</h3>
<p>A relevant question at this moment is how we can find the Jacobian of $F(x)$ using what we learned <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">in this post</a> and <a href="https://sdamadi.github.io/jacobian-of-a-composition-function.html">the other one</a>. We know Jacobian matrix of a composite function is the product of Jacobians so </p>
<p>$$<br>J_F(x) = J_{f}(W^{\top}x+b)J_{W^{\top}x+b}(x) = J_{f}(h)\circ (W^{\top}x+b) J_{W^{\top}x+b}(x)<br>$$</p>
<p>where $h = W^{\top}x+b$. Note that Jacobian of $f$ is being taken with respect to $h$. In neural networks $f$ is a scalar function which is applied to each element of $h$ separately so we have the following</p>
<p>$$<br>f(h) = <br>\begin{bmatrix}<br>f(h_1)\\<br>f(h_2)\\<br>\vdots\\<br>f(h_m)<br>\end{bmatrix}<br>$$</p>
<p>According to what we have discussed <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">here</a>, $J_f(h)$ is all first-order partial derivatives of $f(h)$ as the following</p>
<p>$$<br>J_f(h)= \begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; \frac{\partial f(h_1)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_1)}{\partial h_m}\\<br>\frac{\partial f(h_2)}{\partial h_1} &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_2)}{\partial h_m}\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>\frac{\partial f(h_m)}{\partial h_1} &amp; \frac{\partial f(h_m)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}<br>$$</p>
<p>It is remarkable that because of the structure that we define for the neural network, we get a diagonal matrix for the Jacobian of $J_f(h)$ where each element of its diagonal is functionally the same but is being represented by different variables. However, we are after $J_{f}(h)\circ (W^{\top}x+b)$ which says wherever we have one coordinate of $h$, i.e., $h_i$ where $i = 1, 2, \cdots, m$, substitute the coresponding coordinate of $W^{\top}x+b$.</p>
<p>$$<br>\begin{align}<br>J_f(h)\circ (W^{\top}x+b)<br>&amp;=<br>\begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}\circ (W^{\top}x+b)\\<br>&amp;=<br>\begin{bmatrix}<br>f'(h_1) &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; f'(h_2)&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; f'(h_m)<br>\end{bmatrix}<br>=\phi'_f(h)<br>\end{align}<br>$$</p>
<p>where $\phi'_f(h)$ is defined as a digonal matrix whose diagonal is the derivative of $f$ at the corresponding coordinates of $h$. </p>
<p><a href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html">From this post</a> we know Jacobian matrix of $J_{W^{\top}x+b}(x)$ is just $W^{\top}$.</p>
<p>Therefore, $$<br>J_F(x) = J_{f}(h)\circ (W^{\top}x+b) J_{W^{\top}x+b}(x) = \phi'_f(h)W^{\top}<br>$$</p>
<p>In the following I am going to elaborate on a neural network that has more layer than input and output and derive its Jacobian matrix. Consifer the following network</p>
<figure class="post__image post__image--center" ><img src="https://sdamadi.github.io/media/posts/8/two-layers.png" width="600" height="876">
<figcaption >A nonlinear neural network with more than input and output layers </figcaption>
</figure>
<p> Using the above we can write </p>
<p>$$<br>\begin{align}<br>F(x)&amp;=a_2\\<br>&amp;= f_2(w_2^{\top}a_1+b_2)\\<br>F(x)&amp;=<br>f_2(w_2^{\top}f_1(w_1^{\top}x+b_1)+b_2)\\<br>F(x)&amp;=f_2\circ(w_2^{\top}u+b_2)\circ f_1( w_1^{\top}x+b_1) <br>\end{align}<br>$$</p>
<p>Since $a_1 = w_1^{\top}x+b_1$.</p>
<p>In order to find the Jacobian matrix with respect to the input we have to use the multiplication rule that we derived here. Therefore,</p>
<p>$$<br>\begin{align}<br>J_F(x)&amp;= J_{f_2}(w_2^{\top} a_1 + b_2) J_{f_1}(w_1^{\top} x + b_1)\\<br>&amp;= \phi'_{f_2}(a_2)w_2^{\top}\phi'_{f_1}(a_1)w_1^{\top}<br>\end{align}<br>$$</p>
<p>If you associate the above formula with the figure, you can conclude that no matter how many layers we have, the same pattern would repeat for them.</p>
<p>To recap what we have done so far, we can claim that by looking at the suppressed network, we can find the Jacobian matrix. </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Linear neural network to creat $f(x)&#x3D; W^{\top}x$ </title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/how-to-use-linear-nn-to.html"/>
        <id>https://sdamadi.github.io/how-to-use-linear-nn-to.html</id>

        <updated>2019-12-25T21:36:49-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/7/" alt="" />
                    Goal:In this post we are going to go through the very first step that help us to suppress algebraic notations for representing the function calculating by a fully connected neural network. Following this step helps you to master shorthand notations to express operations happening in&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/7/" alt="" />
                <h3 id="mcetoc_1dsvlh3sp0">Goal:</h3>
<p>In this post we are going to go through the very first step that help us to suppress algebraic notations for representing the function calculating by a fully connected neural network. Following this step helps you to master shorthand notations to express operations happening in the neural network.</p>
<h3 id="mcetoc_1dsvlhtoo1">A linear function in the form of $f(x)= W^{\top}x$</h3>
<p><a href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html">In the previous post</a> we discussed how to find the Jacobian matrix of $f(x)= Ax$ where $A \in \mathbb{R}^{m \times n}$ and $x = [x_1, x_2, \cdots, x_n]^{\top}$. Also, I showed how to represent $f(x)$ in terms of rows of $A$. However, here, we are dealing with $W^{\top}$ in place of $A$ where $W \in \mathbb{R}^{n \times m}$  which requires us to do some work. In order to show how we can build this function using a linear neural network. Suppose $y= f(x)$. Therefore, each coordinate of $f(x)$ equals to each coordinate of a $y$. That is</p>
<p>$$<br>\begin{align}<br>y &amp;= <br>\begin{bmatrix}<br>y_1\\<br>y_2\\<br>\vdots\\<br>y_n<br>\end{bmatrix} =<br>f(x)=W^{\top}x\\<br>&amp;\stackrel{(1)}{=}<br>\begin{bmatrix}<br>W_{\bullet1} &amp; W_{\bullet2} &amp; \cdots &amp; W_{\bullet m}<br>\end{bmatrix}^{\top}x\\<br>&amp;\stackrel{(2)}{=}<br>\begin{bmatrix}<br>W_{\bullet 1}\\<br>W_{\bullet 2}\\\\<br>\vdots\\<br>W_{\bullet m}\\<br>\end{bmatrix}x<br>\stackrel{(3)}{=}<br>\begin{bmatrix}<br>W_{\bullet 1}x\\<br>W_{\bullet 2}x\\\\<br>\vdots\\<br>W_{\bullet m}x\\<br>\end{bmatrix}\\<br>&amp;\stackrel{(4)}{=}<br>\begin{bmatrix}<br>w_{11}x_1 + w_{21}x_2 + \cdots + w_{n1}x_n\\<br>w_{12}x_1 + w_{22}x_2 + \cdots + w_{n2}x_n\\<br>\vdots\\<br>w_{1m}x_1 + w_{2m}x_2 + \cdots + w_{nm}x_n\\<br>\end{bmatrix}<br>\end{align}<br>$$</p>
<p>where $W_{i\bullet}$ is the $i$-th row of $W$ and $W_{\bullet j}$ is the $j$-th column of $W$. Also,</p>
<ul>
<li>(1) is the representation of $W$ columnwise with $m$ columns($W \in \mathbb{R}^{n \times m}$)</li>
<li>(2) is the transpose of $w$ so every column becomes a row</li>
<li>(3) and (4) are because of matrix multiplication rule</li>
</ul>
<p>I had promissed to represent $f(x)= W^{\top}x$ using neural network. To that end, look at the following simple neural network where we have only input and output layers. Input layer has $2$ elements and output layer has $3$ leyers. Each input gets multiplied by its corresponding weight and adds to the other product in order to build each element of $y$.</p>
<figure class="post__image post__image--center" ><img src="https://sdamadi.github.io/media/posts/7/Simplest-neural-network-without-weights.png" alt="Simlest neural network" width="300" height="231">
<figcaption >A simple inear neural network without weight assignments</figcaption>
</figure>
<p>If I annotate every arrow using a weight $w$ whose first index is associated with input and second one with the output we have the following network.</p>
<figure class="post__image post__image--center" ><img src="https://sdamadi.github.io/media/posts/7/NN-With-weights.png" alt="With weights" width="300" height="227">
<figcaption >A simple inear neural network with weight assignments</figcaption>
</figure>
<p> If you carefully look, we can write each element of $y$ using linear combination of $x$'s element and $w$'s as the following</p>
<p>$$<br>\begin{align}<br>y &amp;= <br>\begin{bmatrix}<br>y_1\\<br>y_2\\<br>y_3\\<br>\end{bmatrix} =<br>\begin{bmatrix}<br>W_{11}x_1 + W_{21}x_2 \\<br>W_{12}x_1 + W_{22}x_2 n\\<br>W_{13}x_1 + W_{23}x_2 \\<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix}<br>W_{11} &amp; W_{21}\\<br>W_{12} &amp; W_{22}\\<br>W_{13} &amp; W_{23} \\<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1\\<br>x_2\\<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix}<br>W_{11} &amp; W_{12} &amp; W_{13}\\<br>W_{21} &amp; W_{22} &amp; W_{23}\\<br>\end{bmatrix}^{\top}<br>\begin{bmatrix}<br>x_1\\<br>x_2\\<br>\end{bmatrix}<br>=W^{\top}x = f(x)<br>\end{align}<br>$$<br><br></p>
<p>Hence, from now on, I will assign $W$ between each two layers because we can think of them as input($x$) and output($y$) where output formula is $y = W^{\top}x$.</p>
<figure class="post__image post__image--center" ><img src="https://sdamadi.github.io/media/posts/7/Single-leyer-Single-Weight.png" alt="One weight" width="300" height="659">
<figcaption >Shorthand notation</figcaption>
</figure>
<p>In the next post I will add another element to our function which is called biass and because of that our function would be $W^{\top}x + b$.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Compute the gradient of a neural network</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html"/>
        <id>https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html</id>

        <updated>2019-12-25T14:58:08-05:00</updated>
            <summary></summary>
        <content></content>
    </entry>
    <entry>
        <title>What is Jacobian matrix of $f(x)&#x3D; Ax$?</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html"/>
        <id>https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html</id>

        <updated>2019-12-25T17:55:43-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/5/" alt="" />
                    In this short post we are going to find the Jacobian matrix of $f(x)= Ax$ where $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, $x \in \mathbb{R}^n$, and $A \in \mathbb{R}^{m \times n}$. As I explained here, in order to find the Jacobian matrix we need a vector-valued function&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/5/" alt="" />
                <p>In this short post we are going to find the Jacobian matrix of $f(x)= Ax$ where $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, $x \in \mathbb{R}^n$, and $A \in \mathbb{R}^{m \times n}$.</p>
<h3 id="mcetoc_1dsvafce90">Why this function is so important?</h3>
<ol>
<li>It is a linear function</li>
<li>It appers in neural networks when we want to find <a href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html">the loss function</a></li>
<li>It generalize $f(x)= ax$ where $x$, and $a$ are scalars.</li>
</ol>
<p><a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html#mcetoc_1dsqmf85h2">As I explained here</a>, in order to find the Jacobian matrix we need a vector-valued function which we have but one should be able to represent each coordinte of $f$ as a function so we do the following:</p>
<p>$$<br>\begin{align}<br>f(x)&amp;= Ax =<br>\begin{bmatrix}<br>A_{1\bullet}\\<br>A_{2\bullet}\\<br>\vdots\\<br>A_{m\bullet}\\<br>\end{bmatrix}<br>x<br>=<br>\begin{bmatrix}<br>A_{1\bullet}x\\<br>A_{2\bullet}x\\<br>\vdots\\<br>A_{m\bullet}x\\<br>\end{bmatrix}=<br>\begin{bmatrix}<br>f_1(x)\\<br>f_2(x)\\<br>\vdots\\<br>f_n(x)\\<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix}<br>a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n\\<br>a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n\\<br>\vdots\\<br>a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_nx\\<br>\end{bmatrix}\\<br>\end{align}<br>$$</p>
<p>where $A_{i\bullet}$ is the $i$-th row of $A$ and $x = [x_1, x_2, \cdots, x_n]^{\top}$.<br>Hence according to what we discussed <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">in the other post</a>,<br>$$<br>J_f(x) = \begin{bmatrix}<br>a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\<br>a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\<br>\end{bmatrix}<br>=A<br>$$</p>
<p><strong>Note</strong>: It does not make any differences if we add a constant vector $b$ to $Ax$, i.e., $f(x) = Ax + b$.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>What is Jacobian matrix and why do we need it?</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html"/>
        <id>https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html</id>

        <updated>2019-12-23T21:31:52-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/4/" alt="" />
                    Derivative of univariate functionTo understand what is Jacobian, we need to revisit the derivative of a univariate function wherein $f$ maps the real line into the real line, that is, $f: \mathbb{R} \rightarrow \mathbb{R} $. The derivative of $f$ denoted by $f'$ measures the sensitivity to change&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/4/" alt="" />
                <h3 id="mcetoc_1dsqkbs450">Derivative of univariate function</h3>
<p>To understand what is Jacobian, we need to revisit the derivative of a univariate function wherein $f$ maps the real line into the real line, that is, $f: \mathbb{R} \rightarrow \mathbb{R} $. The derivative of $f$ denoted by $f'$ measures the sensitivity to change of the function value (output value, $f(x)$) with respect to a change in its argument (input value, $x$). Now the question is how we can measure the change of a function whose inputs is a vector in $\mathbb{R}^n$. From this point on we will focus on the change in a value of a function and try to associate it to the derivative of a univariate function.</p>
<p><strong>Note</strong>: $f'$ is also denoted by $\frac{df}{dx}$ which is read as the derivative of $f$ with respect to $x$. Here we have used $d$ since there is no other variables that derivative is being taken with respect to them.</p>
<p><strong>Remark: </strong>If $x$ becomes a number in real line instead of a vector, the gradient becomes the derivative which was expected.</p>
<h3 id="mcetoc_1dsqkdumb1">The gradient of a real-valued function</h3>
<p>When $f$ takes on a vector $x \in \mathbb{R}^n$ where $x = [x_1, x_2, \cdots,x_n]^{\top}$ and maps it to a number on the real line, we write $f : \mathbb{R}^n \rightarrow \mathbb{R}$. Since we have the ability to play with each coordinate of $x$ to change the value of the function, the concept of gradient comes into the picture. The gradient is a vector with the same length of $x$ and each coordinate shows the change in the value of $f$ when the corresponding coordinate changes. Therefore, we write</p>
<p> \begin{equation}<br>\nabla f = [\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}]^{\top}<br>\end{equation}</p>
<p>Here we use $\nabla$ (nabla) symbol to suppress the right hand side when we want to address all the changes along $n$ directions. Also, $\partial$ is used to clarify that other than (let's say $x_1$), there are other variables whose change can affect the value of $f$.</p>
<h3 id="mcetoc_1dsqmf85h2">Jacobian of a vector-valued function</h3>
<p>Now let's get back to the question that I asked in the page title. You might say, "well, Jacobian matrix should be something that somehow connects to the change of a vector-valued function which takes a vector as its input". Yes correct! But how we do we write it?</p>
<p>First notice that we have $f$ where $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, that is:</p>
<p>$$<br> f(x)=f(x_1, x_2, \cdots,x_n)= \begin{bmatrix}<br> f_1(x_1, x_2, \cdots,x_n)\\ f_2(x_1, x_2, \cdots,x_n)\\ \vdots \\ f_n(x_1, x_2, \cdots,x_n)<br> \end{bmatrix}<br>$$</p>
<p>As you can see the value of each function $f_i$ where $i= 1, 2, \cdots, m$ can vary when one of $x_j$'s changes. This is where Jacobian matrix comes into the picture. It is defined as the following:</p>
<p>$$<br>J_f(x)= \begin{bmatrix}<br>\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2}&amp; \cdots&amp; \frac{\partial f_1}{\partial x_n}\\<br>\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2}&amp; \cdots&amp; \frac{\partial f_2}{\partial x_n}\\<br> \vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>\frac{\partial f_m}{\partial x_1} &amp; \frac{\partial f_m}{\partial x_2}&amp; \cdots&amp; \frac{\partial f_m}{\partial x_n}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\nabla^{\top} f_1\\<br>\nabla^{\top} f_2\\<br>\vdots\\<br>\nabla^{\top} f_n\\<br>\end{bmatrix}<br>$$</p>
<p>By looking at $i$-th row of the Jacobian matrix, we see that it is transpose of the gradient of $f_i$, i.e., $(\nabla f_i)^{\top}$ which I denoted by $\nabla^{\top} f_i$ . </p>
<p><strong>Remark 1</strong>: If $f$ becomes a scalar-valued function and $x$ becomes a number in real line instead of a vector, the Jacobian matrix becomes the derivative!</p>
<p><strong>Remark 2</strong>: Jacobian matrix is the matrix of all first-order partial derivatives of the function.</p>
<h3 id="mcetoc_1dsqpjd0t0">Why do we need Jacobian matrix?</h3>
<p>Jacobian of a composition function is the product of the Jacobian matrices of each functions. Suppose we have the following fucntion:</p>
<p>$$<br>f (x)= f_1(f_2(\cdots( f_n(x) ) ) = f_1 \circ f_2 \circ \cdots\circ f_n(x)<br>$$</p>
<p>The Jacobian matrix of $f$ with respect to $x$ is:</p>
<p>$$<br>J_f = J_{f_1} \cdot J_{f_2} \cdots J_{f_n}(x)<br>$$</p>
<p>However, there is a big caveat here; in order to find Jacobian matrix of $f$, we need to find all other Jacobian matrices. We know Jacobian matrix is all first-order partial derivatives of the vector-valued function with respect to its variable. But what is the variable of, let's say $f_1$? The variable of $f_1$ is the f but $f_1$, i.e., $f_2(f_3(\cdots( f_n(x) ) ) = f_2 \circ f_3 \circ \cdots\circ f_n(x)$. </p>
<p><a href="https://sdamadi.github.io/jacobian-of-a-composition-function.html">In this post</a> I will go through an example and clarify all those abstract concepts that were discussed in the last paragraph.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Jacobian matrix of a composite function</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/jacobian-of-a-composition-function.html"/>
        <id>https://sdamadi.github.io/jacobian-of-a-composition-function.html</id>

        <updated>2019-12-25T15:08:55-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/3/" alt="" />
                    This post is the continuation of what I have discussed here to clarify what is Jacobian matrix. We are going to see an example in which Jacobian matrix is being applied on a composite function including two functions. This case is very intuitive since it&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/3/" alt="" />
                <p>This post is the continuation of what I have discussed <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">here</a> to clarify what is Jacobian matrix. We are going to see an example in which Jacobian matrix is being applied on a composite function including two functions. This case is very intuitive since it will shed light on the general case. Using what we will find here, the general case will be done.</p>
<p>Consider $h(x)=f(g(x))$ where $x \in \mathbb{R}^n$, $g:  \mathbb{R}^n \rightarrow  \mathbb{R}^\ell$, and $f:  \mathbb{R}^{\ell} \rightarrow  \mathbb{R}^m$.</p>
<p>Hence, $$h:  \mathbb{R}^{n} \rightarrow  \mathbb{R}^m$$</p>
<p>According to what I explained <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html#mcetoc_1dsr1mght0">here</a>, Jacobian matrix of $h$ is an $m \times n$.</p>
<p>Also,</p>
<p>$$ <br>J_h(x) = J_f(g(x))J_g(x)<br> $$ </p>
<p>where $J_g(x)$ is an $\ell \times n$ and $J_f(g(x))$ is an $m \times \ell $ matrix.</p>
<p><strong>Remark</strong>:</p>
<ul>
<li id="1b">$J_g(x)$ is an $\ell \times n$ matrix because it takes an $n$ dimensional vector and maps it to $\ell$ dimensional vector.</li>
<li>$J_f(g(x))$ is an $m \times \ell$ matrix because its input $g(x)$ is an $\ell$ dimensional vector and the output of $f$ is an $m$ dimensional vector.</li>
<li id="3b">Since the number of columns of $J_f(g(x))$ is the same as the number of rows of $J_g(x)$  they are comformable and the product is an $m \times n$ matrix.</li>
<li id="4b">$J_g(x)$ is simply Jacobian matrix of a vector-valued function which was defined in <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">the other post</a>.</li>
<li id="5b">To find $J_f(g(x))$, it is required to have $f$ in terms of each coordinate of $g(x)$ in order to be able to find all the first-order partial derivatives. If $f$ and $g$ are given separately as two functions like $f(z)$ and $g(x)$, to find $J_f(g(x))$ it suffice to find the jacobian of $f$ with respect to $z$ then substitute coordinates of $g(x)$ in as the coordinates of $z$. Precisely, we have $J_f(z) \circ g(x)$.</li>
<li id="6b">When $h(x)$ is given as a function of $x$, then we are back to the case where Jacobian matrix can be found as the way we found it for a vector-valued fucntion in <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">the other post</a>.</li>
</ul>
<p>In what follows, I will walk you through what I discussed for the above case. Suppose we are given</p>
<p>$f\left(u,v\right)=u^{2}+3v^{2}$,</p>
<p>$g\left(x,y\right)=\begin{bmatrix} e^{x}\cos y  \\ e^{x}\sin y \end{bmatrix} $,</p>
<p>and the goal is finding the Jacobian matrix of the composite function.  Also, let the composite function be $h$.</p>
<p><strong>Question: </strong>$h$ is the composite function of $f$ and $g$ or $g$ and $f$?</p>
<p><strong>Answer: </strong>To clarify the above question, we are required to think of $f$ and $g$ as two mappings where $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ and $g: \mathbb{R}^2 \rightarrow \mathbb{R}^2$.</p>
<p>Suppose $h = f \circ g$.</p>
<p>Since $g$ is the last function in the composite function, variables of $h$ are going to be vaibales of $g$. Therefore, $J_h(x, y) = J_f(g(u,v))J_g(x, y)$. <a href="#1b">According to the first and second bullet</a>, $J_f(g(u,v))$ is a $1 \times 2$ matrix and $J_g(x, y)$ is a $2 \times 2$ matrix. Hence, the result is a $1 \times 2$ matrix. However, if we flip $f$ and $g$, and consider $h$ as $h=g \circ f$ and we should have</p>
<p>$$J_h(u, v) \stackrel{?}{=} J_g(f(u,v))J_f(u, y)$$</p>
<p>It is easy to see <a href="#1b">from the first bullet</a> that we have a $1 \times 2$ for $J_f(u, v)$ matrix but $J_g(f(x,y))$ is not defind since one should take the derivatives of all coordinates of $g$ with respect to all varibales of $g$ which are now coordinates of $f$; but $f$ is a scalar-valued function and does not output the same dimension as $g$ input needs. In effect, $h=g \circ f$ is not defined. Hence, by compostion function, we mean $h = f \circ g$.</p>
<p>Let's first compute $J_g(x, y)$ where $g$ is a function of $x,y$ and it is a vector-valued function so</p>
<p>$$<br> J_g(x, y) = \begin{bmatrix}<br>\frac{\partial g_1}{\partial x} &amp; \frac{\partial g_1}{\partial y} \\<br>\frac{\partial g_2}{\partial x} &amp; \frac{\partial g_2}{\partial y} <br> \end{bmatrix}=<br>\begin{bmatrix}<br>e^x\cos y &amp; -e^x\sin y \\ e^x\sin y &amp; e^x\cos y<br>\end{bmatrix}$$  </p>
<p>Now to compute $J_f(g(u,v))$ <a href="#5b">using the fifth bullet</a> which yields<br>$$<br>\begin{align}<br>J_f(g(u,v)) &amp;= J_f(z = (u,v))\circ g(x,y)\\<br> &amp;= <br>\begin{bmatrix}<br>\frac{\partial f}{\partial u} &amp; \frac{\partial f}{\partial v}<br> \end{bmatrix} \circ g(x,y) \\<br>&amp;=\begin{bmatrix}<br>2u &amp; 6v<br>\end{bmatrix}\circ g(x,y)\\<br>&amp;=\begin{bmatrix}<br>2u &amp; 6v<br>\end{bmatrix}\circ<br>\begin{bmatrix}<br>e^{x}\cos y \\ e^{x}\sin y<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix} 2e^{x}\cos y &amp; 6e^{x}\sin y \end{bmatrix}<br>\end{align}<br>$$<br><br>Finally,</p>
<p>$$<br>\begin{align}<br>J_{h}(x,y) &amp;= J_{f \circ g}(x,y) = J_f(g(u,v)) J_g(x, y)\\<br>&amp;=<br>\begin{bmatrix} 2e^{x}\cos y &amp; 6e^{x}\sin y \end{bmatrix}<br>\begin{bmatrix}<br>e^x\cos y &amp; -e^x\sin y \\ e^x\sin y &amp; e^x\cos y<br>\end{bmatrix}\\<br>&amp;= <br>\begin{bmatrix}<br>2e^{2x}\cos^2y + 6e^{2x}\sin^2y &amp; -2e^{2x}\cos y \sin y + 6e^{2x}\sin y\cos y<br>\end{bmatrix} \\<br>&amp;= <br>2e^{2x}<br>\begin{bmatrix}<br>1 + 2\sin^2y &amp; 2\sin y\cos y <br>\end{bmatrix}<br>\end{align}\\<br>$$</p>
<p> <strong>Conclusion: </strong>We went through all the steps of finding the Jacobian matrix of a composite function consists of two functions. It is one step left to apply all these result to <a href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html">the lsos function of a neural network</a>.</p>
<p> </p>
            ]]>
        </content>
    </entry>
</feed>
