<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Creating nonlinear neural network and finding the Jacobian matrix of its funciton - Saeed Damadi</title><meta name="description" content="The next step towards finding loss function of a neural network is to extend the results we found here to add a bias to the linear function, i.e., creating $W^{\top}x+b$ where $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, and $W \in \mathbb{R}^{n \times m}$. This can be done easily because&hellip;"><meta name="robots" content="index, follow"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"><link rel="amphtml" href="https://sdamadi.github.io/amp/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"><link rel="alternate" type="application/atom+xml" href="https://sdamadi.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://sdamadi.github.io/feed.json"><meta property="og:title" content="Creating nonlinear neural network and finding the Jacobian matrix of its funciton"><meta property="og:site_name" content="My blog - Saeed Damadi"><meta property="og:description" content="The next step towards finding loss function of a neural network is to extend the results we found here to add a bias to the linear function, i.e., creating $W^{\top}x+b$ where $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, and $W \in \mathbb{R}^{n \times m}$. This can be done easily because&hellip;"><meta property="og:url" content="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"><meta property="og:type" content="article"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin><link href="https://fonts.googleapis.com/css?family=Roboto:400,500&amp;subset=latin-ext" rel="stylesheet"><link rel="stylesheet" href="https://sdamadi.github.io/assets/css/style.css?v=b51349db1483749b4e0ec880159fc912"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"},"headline":"Creating nonlinear neural network and finding the Jacobian matrix of its funciton","datePublished":"2019-12-25T20:12","dateModified":"2019-12-28T20:28","description":"The next step towards finding loss function of a neural network is to extend the results we found here to add a bias to the linear function, i.e., creating $W^{\\top}x+b$ where $x \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}^m$, and $W \\in \\mathbb{R}^{n \\times m}$. This can be done easily because&hellip;","author":{"@type":"Person","name":"S. M. Saeed Damadi"},"publisher":{"@type":"Organization","name":"S. M. Saeed Damadi"}}</script><script async src="https://sdamadi.github.io/assets/js/lazysizes.min.js?v=dc4b666bb3324aea4ead22e26059c761"></script><style>.infobar__search [type="search"] {
	                     background-image: url(https://sdamadi.github.io/assets/svg/search.svg);
	                 }</style><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script></head><body><header class="topbar js-top is-sticky"><div class="topbar__inner"><a class="logo" href="https://sdamadi.github.io/">Saeed Damadi</a><nav class="navbar js-navbar"><button class="navbar__toggle js-navbar__toggle">Menu</button><ul class="navbar__menu"></ul></nav></div></header><div class="content"><div class="infobar"><div class="infobar__update">Last Updated: <time datetime="2021-02-17T01:36">February 17, 2021</time></div><div class="infobar__search"><form action="https://sdamadi.github.io/search.html"><input type="search" name="q" placeholder=" search..."></form></div></div><main class="main"><article class="post"><header class="u-header post__header"><h1>Creating nonlinear neural network and finding the Jacobian matrix of its funciton</h1><div class="u-header__meta u-small"><div><a href="https://sdamadi.github.io/authors/saeed/" title="S. M. Saeed Damadi">S. M. Saeed Damadi</a> <time datetime="2019-12-25T20:12">December 25, 2019</time></div></div></header><div class="post__entry u-inner"><p>The next step towards finding loss function of a neural network is to extend the results we found <a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html">here</a> to add a bias to the linear function, i.e., creating $W^{\top}x+b$ where $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, and $W \in \mathbb{R}^{n \times m}$. This can be done easily because we already have the part of a network that generates $W^{\top}x$. It suffice to add a constant vector to the previous result. Therefore, we have the follwing for the case where $n = 2$ and $m = 3$</p><figure class="post__image post__image--center"><img class="lazyload" src="https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-xs.png" data-sizes="auto" data-srcset="https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-xs.png 300w ,https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-sm.png 480w ,https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-md.png 768w ,https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-lg.png 1024w ,https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-xl.png 1360w ,https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-2xl.png 1600w" alt="" width="300" height="628"><figcaption>Adding bias to the neural network</figcaption></figure><p>Now to create a nonlinear function $F(x)=f( W^{\top}x+b)$, we need to pass in the above quantity to the function $f$, which produces $F(x)$.</p><h3> Jacobian matrix of $F(x)=f( W^{\top}x+b)$</h3><p>A relevant question at this moment is how we can find the Jacobian of $F(x)$ using what we learned <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">in this post</a> and <a href="https://sdamadi.github.io/jacobian-of-a-composition-function.html">the other one</a>. We know Jacobian matrix of a composite function is the product of Jacobians so </p><p>$$<br>J_F(x) = J_{f}(W^{\top}x+b)J_{W^{\top}x+b}(x) = J_{f}(h)\circ (W^{\top}x+b) J_{W^{\top}x+b}(x)<br>$$</p><p>where $h = W^{\top}x+b$. Note that Jacobian of $f$ is being taken with respect to $h$. In neural networks $f$ is a scalar function which is applied to each element of $h$ separately so we have the following</p><p>$$<br>f(h) =<br>\begin{bmatrix}<br>f(h_1)\\<br>f(h_2)\\<br>\vdots\\<br>f(h_m)<br>\end{bmatrix}<br>$$</p><p>According to what we have discussed <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">here</a>, $J_f(h)$ is all first-order partial derivatives of $f(h)$ as the following</p><p>$$<br>J_f(h)= \begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; \frac{\partial f(h_1)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_1)}{\partial h_m}\\<br>\frac{\partial f(h_2)}{\partial h_1} &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_2)}{\partial h_m}\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>\frac{\partial f(h_m)}{\partial h_1} &amp; \frac{\partial f(h_m)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}<br>$$</p><p>It is remarkable that because of the structure that we define for the neural network, we get a diagonal matrix for the Jacobian of $J_f(h)$ where each element of its diagonal is functionally the same but is being represented by different variables. However, we are after $J_{f}(h)\circ (W^{\top}x+b)$ which says wherever we have one coordinate of $h$, i.e., $h_i$ where $i = 1, 2, \cdots, m$, substitute the coresponding coordinate of $W^{\top}x+b$.</p><p>$$<br>\begin{align}<br>J_f(h)\circ (W^{\top}x+b)<br>&amp;=<br>\begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}\circ (W^{\top}x+b)\\<br>&amp;=<br>\begin{bmatrix}<br>f'(h_1) &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; f'(h_2)&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; f'(h_m)<br>\end{bmatrix}<br>=\phi'_f(h)<br>\end{align}<br>$$</p><p>where $\phi'_f(h)$ is defined as a digonal matrix whose diagonal is the derivative of $f$ at the corresponding coordinates of $h$. </p><p><a href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html">From this post</a> we know Jacobian matrix of $J_{W^{\top}x+b}(x)$ is just $W^{\top}$.</p><p>Therefore, $$<br>J_F(x) = J_{f}(h)\circ (W^{\top}x+b) J_{W^{\top}x+b}(x) = \phi'_f(h)W^{\top}<br>$$</p><p>In the following I am going to elaborate on a neural network that has more layer than input and output and derive its Jacobian matrix. Consifer the following network</p><figure class="post__image post__image--center"><img class="lazyload" src="https://sdamadi.github.io/media/posts/8/responsive/two-layers-xs.png" data-sizes="auto" data-srcset="https://sdamadi.github.io/media/posts/8/responsive/two-layers-xs.png 300w ,https://sdamadi.github.io/media/posts/8/responsive/two-layers-sm.png 480w ,https://sdamadi.github.io/media/posts/8/responsive/two-layers-md.png 768w ,https://sdamadi.github.io/media/posts/8/responsive/two-layers-lg.png 1024w ,https://sdamadi.github.io/media/posts/8/responsive/two-layers-xl.png 1360w ,https://sdamadi.github.io/media/posts/8/responsive/two-layers-2xl.png 1600w" width="600" height="876"><figcaption>A nonlinear neural network with more than input and output layers </figcaption></figure><p> Using the above we can write </p><p>$$<br>\begin{align}<br>F(x)&amp;=a_2\\<br>&amp;= f_2(w_2^{\top}a_1+b_2)\\<br>F(x)&amp;=<br>f_2(w_2^{\top}f_1(w_1^{\top}x+b_1)+b_2)\\<br>F(x)&amp;=f_2\circ(w_2^{\top}u+b_2)\circ f_1( w_1^{\top}x+b_1) <br>\end{align}<br>$$</p><p>Since $a_1 = w_1^{\top}x+b_1$.</p><p>In order to find the Jacobian matrix with respect to the input we have to use the multiplication rule that we derived here. Therefore,</p><p>$$<br>\begin{align}<br>J_F(x)&amp;= J_{f_2}(w_2^{\top} a_1 + b_2) J_{f_1}(w_1^{\top} x + b_1)\\<br>&amp;= \phi'_{f_2}(a_2)w_2^{\top}\phi'_{f_1}(a_1)w_1^{\top}<br>\end{align}<br>$$</p><p>If you associate the above formula with the figure, you can conclude that no matter how many layers we have, the same pattern would repeat for them.</p><p>To recap what we have done so far, we can claim that by looking at the suppressed network, we can find the Jacobian matrix. </p></div><aside class="post__aside"><div class="post__last-updated u-small">This article was updated on December 28, 2019</div><div class="post__share"></div></aside><footer class="post__footer"><div class="post__bio u-author box"><div><h4 class="u-author__name"><a href="https://sdamadi.github.io/authors/saeed/" title="S. M. Saeed Damadi">S. M. Saeed Damadi</a></h4></div></div><nav class="post__nav box"><div class="post__nav__prev"><a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html" class="post__nav__link" rel="prev"><div class="u-small">Previous Post<h5>Linear neural network to creat $f(x)&#x3D; W^{\top}x$</h5></div></a></div></nav><div class="post__related box"><h3 class="box__title">Related posts</h3><div class="post__related__wrap"><figure><figcaption><h4><a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html" class="inverse">Linear neural network to creat $f(x)&#x3D; W^{\top}x$</a></h4><time datetime="2019-12-25T17:57" class="u-small">December 25, 2019</time></figcaption></figure><figure><figcaption><h4><a href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html" class="inverse">Compute the gradient of a neural network</a></h4><time datetime="2019-12-25T14:57" class="u-small">December 25, 2019</time></figcaption></figure><figure><figcaption><h4><a href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html" class="inverse">What is Jacobian matrix of $f(x)&#x3D; Ax$?</a></h4><time datetime="2019-12-25T14:54" class="u-small">December 25, 2019</time></figcaption></figure></div></div></footer></article></main><div class="sidebar"><section class="box"><h3 class="box__title">Authors</h3><ul class="authors"><li><a href="https://sdamadi.github.io/authors/saeed/"><img data-src="" class="lazyload authors__img" alt="S. M. Saeed Damadi"></a><div><a href="https://sdamadi.github.io/authors/saeed/" class="authors__title">S. M. Saeed Damadi</a> <span class="u-small">Post: 6</span></div></li></ul></section><section class="newsletter box box--gray"><h3 class="box__title">Newsletter</h3><p class="newsletter__description">Sign up to receive email updates and to hear what's going on with us!</p><form>...</form></section></div><footer class="footer"><a class="footer__logo" href="https://sdamadi.github.io/">Saeed Damadi</a><nav><ul class="footer__nav"></ul></nav><div class="footer__copyright">Powered by Publii</div></footer></div><script defer="defer" src="https://sdamadi.github.io/assets/js/jquery-3.2.1.slim.min.js?v=f307ecc7f6353949f03c06ffa70652a2"></script><script defer="defer" src="https://sdamadi.github.io/assets/js/scripts.min.js?v=07b53ae91f8045eab042a695e4a26034"></script></body></html>