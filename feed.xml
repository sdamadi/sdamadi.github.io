<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Saeed Damadi</title>
    <link href="https://sdamadi.github.io/feed.xml" rel="self" />
    <link href="https://sdamadi.github.io" />
    <updated>2019-12-25T15:06:01-05:00</updated>
    <author>
        <name>Saeed</name>
    </author>
    <id>https://sdamadi.github.io</id>

    <entry>
        <title>Compute the gradient of a neural network</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html"/>
        <id>https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html</id>

        <updated>2019-12-25T14:58:08-05:00</updated>
            <summary></summary>
        <content></content>
    </entry>
    <entry>
        <title>What is Jacobian matrix of $f(x)&#x3D; Ax$?</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html"/>
        <id>https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html</id>

        <updated>2019-12-25T15:05:45-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/5/" alt="" />
                    In this short post we are going to find the Jacobian matrix of $f(x)= Ax$ where $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, $x \in \mathbb{R}^n$, and $A \in \mathbb{R}^{m \times n}$. 
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/5/" alt="" />
                <p>In this short post we are going to find the Jacobian matrix of $f(x)= Ax$ where $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, $x \in \mathbb{R}^n$, and $A \in \mathbb{R}^{m \times n}$.</p>
<h3 id="mcetoc_1dsvafce90">Why this function is so important?</h3>
<ol>
<li>It is a linear function</li>
<li>It will apper in neural network when we want to find the loss function</li>
<li>It generalize $f(x)= ax$ where $x$, and $a$ are scalars.</li>
</ol>
<p> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>What is Jacobian matrix and why do we need it?</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html"/>
        <id>https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html</id>

        <updated>2019-12-23T21:31:52-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/4/" alt="" />
                    Derivative of univariate functionTo understand what is Jacobian, we need to revisit the derivative of a univariate function wherein $f$ maps the real line into the real line, that is, $f: \mathbb{R} \rightarrow \mathbb{R} $. The derivative of $f$ denoted by $f'$ measures the sensitivity to change&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/4/" alt="" />
                <h3 id="mcetoc_1dsqkbs450">Derivative of univariate function</h3>
<p>To understand what is Jacobian, we need to revisit the derivative of a univariate function wherein $f$ maps the real line into the real line, that is, $f: \mathbb{R} \rightarrow \mathbb{R} $. The derivative of $f$ denoted by $f'$ measures the sensitivity to change of the function value (output value, $f(x)$) with respect to a change in its argument (input value, $x$). Now the question is how we can measure the change of a function whose inputs is a vector in $\mathbb{R}^n$. From this point on we will focus on the change in a value of a function and try to associate it to the derivative of a univariate function.</p>
<p><strong>Note</strong>: $f'$ is also denoted by $\frac{df}{dx}$ which is read as the derivative of $f$ with respect to $x$. Here we have used $d$ since there is no other variables that derivative is being taken with respect to them.</p>
<p><strong>Remark: </strong>If $x$ becomes a number in real line instead of a vector, the gradient becomes the derivative which was expected.</p>
<h3 id="mcetoc_1dsqkdumb1">The gradient of a real-valued function</h3>
<p>When $f$ takes on a vector $x \in \mathbb{R}^n$ where $x = [x_1, x_2, \cdots,x_n]^{\top}$ and maps it to a number on the real line, we write $f : \mathbb{R}^n \rightarrow \mathbb{R}$. Since we have the ability to play with each coordinate of $x$ to change the value of the function, the concept of gradient comes into the picture. The gradient is a vector with the same length of $x$ and each coordinate shows the change in the value of $f$ when the corresponding coordinate changes. Therefore, we write</p>
<p> \begin{equation}<br>\nabla f = [\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}]^{\top}<br>\end{equation}</p>
<p>Here we use $\nabla$ (nabla) symbol to suppress the right hand side when we want to address all the changes along $n$ directions. Also, $\partial$ is used to clarify that other than (let's say $x_1$), there are other variables whose change can affect the value of $f$.</p>
<h3 id="mcetoc_1dsqmf85h2">Jacobian of a vector-valued function</h3>
<p>Now let's get back to the question that I asked in the page title. You might say, "well, Jacobian matrix should be something that somehow connects to the change of a vector-valued function which takes a vector as its input". Yes correct! But how we do we write it?</p>
<p>First notice that we have $f$ where $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, that is:</p>
<p>$$<br> f(x)=f(x_1, x_2, \cdots,x_n)= \begin{bmatrix}<br> f_1(x_1, x_2, \cdots,x_n)\\ f_2(x_1, x_2, \cdots,x_n)\\ \vdots \\ f_n(x_1, x_2, \cdots,x_n)<br> \end{bmatrix}<br>$$</p>
<p>As you can see the value of each function $f_i$ where $i= 1, 2, \cdots, m$ can vary when one of $x_j$'s changes. This is where Jacobian matrix comes into the picture. It is defined as the following:</p>
<p>$$<br>J_f(x)= \begin{bmatrix}<br>\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2}&amp; \cdots&amp; \frac{\partial f_1}{\partial x_n}\\<br>\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2}&amp; \cdots&amp; \frac{\partial f_2}{\partial x_n}\\<br> \vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>\frac{\partial f_m}{\partial x_1} &amp; \frac{\partial f_m}{\partial x_2}&amp; \cdots&amp; \frac{\partial f_m}{\partial x_n}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\nabla^{\top} f_1\\<br>\nabla^{\top} f_2\\<br>\vdots\\<br>\nabla^{\top} f_n\\<br>\end{bmatrix}<br>$$</p>
<p>By looking at $i$-th row of the Jacobian matrix, we see that it is transpose of the gradient of $f_i$, i.e., $(\nabla f_i)^{\top}$ which I denoted by $\nabla^{\top} f_i$ . </p>
<p><strong>Remark 1</strong>: If $f$ becomes a scalar-valued function and $x$ becomes a number in real line instead of a vector, the Jacobian matrix becomes the derivative!</p>
<p><strong>Remark 2</strong>: Jacobian matrix is the matrix of all first-order partial derivatives of the function.</p>
<h3 id="mcetoc_1dsqpjd0t0">Why do we need Jacobian matrix?</h3>
<p>Jacobian of a composition function is the product of the Jacobian matrices of each functions. Suppose we have the following fucntion:</p>
<p>$$<br>f (x)= f_1(f_2(\cdots( f_n(x) ) ) = f_1 \circ f_2 \circ \cdots\circ f_n(x)<br>$$</p>
<p>The Jacobian matrix of $f$ with respect to $x$ is:</p>
<p>$$<br>J_f = J_{f_1} \cdot J_{f_2} \cdots J_{f_n}(x)<br>$$</p>
<p>However, there is a big caveat here; in order to find Jacobian matrix of $f$, we need to find all other Jacobian matrices. We know Jacobian matrix is all first-order partial derivatives of the vector-valued function with respect to its variable. But what is the variable of, let's say $f_1$? The variable of $f_1$ is the f but $f_1$, i.e., $f_2(f_3(\cdots( f_n(x) ) ) = f_2 \circ f_3 \circ \cdots\circ f_n(x)$. </p>
<p><a href="https://sdamadi.github.io/jacobian-of-a-composition-function.html">In this post</a> I will go through an example and clarify all those abstract concepts that were discussed in the last paragraph.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Jacobian matrix of a composite function</title>
        <author>
            <name>Saeed</name>
        </author>
        <link href="https://sdamadi.github.io/jacobian-of-a-composition-function.html"/>
        <id>https://sdamadi.github.io/jacobian-of-a-composition-function.html</id>

        <updated>2019-12-25T14:59:09-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://sdamadi.github.io/media/posts/3/" alt="" />
                    This post is the continuation of what I have discussed here to clarify what is Jacobian matrix. We are going to see an example in which Jacobian matrix is being applied on a composite function including two functions. This case is very intuitive since it&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <img src="https://sdamadi.github.io/media/posts/3/" alt="" />
                <p>This post is the continuation of what I have discussed <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">here</a> to clarify what is Jacobian matrix. We are going to see an example in which Jacobian matrix is being applied on a composite function including two functions. This case is very intuitive since it will shed light on the general case. Using what we will find here, the general case will be done.</p>
<p>Consider $h(x)=f(g(x))$ where $x \in \mathbb{R}^n$, $g:  \mathbb{R}^n \rightarrow  \mathbb{R}^\ell$, and $f:  \mathbb{R}^{\ell} \rightarrow  \mathbb{R}^m$.</p>
<p>Hence, $$h:  \mathbb{R}^{n} \rightarrow  \mathbb{R}^m$$</p>
<p>According to what I explained <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html#mcetoc_1dsr1mght0">here</a>, Jacobian matrix of $h$ is an $m \times n$.</p>
<p>Also,</p>
<p>$$ <br>J_h(x) = J_f(g(x))J_g(x)<br> $$ </p>
<p>where $J_g(x)$ is an $\ell \times n$ and $J_f(g(x))$ is an $m \times \ell $ matrix.</p>
<p><strong>Remark</strong>:</p>
<ul>
<li id="1b">$J_g(x)$ is an $\ell \times n$ matrix because it takes an $n$ dimensional vector and maps it to $\ell$ dimensional vector.</li>
<li>$J_f(g(x))$ is an $m \times \ell$ matrix because its input $g(x)$ is an $\ell$ dimensional vector and the output of $f$ is an $m$ dimensional vector.</li>
<li id="3b">Since the number of columns of $J_f(g(x))$ is the same as the number of rows of $J_g(x)$  they are comformable and the product is an $m \times n$ matrix.</li>
<li id="4b">$J_g(x)$ is simply Jacobian matrix of a vector-valued function which was defined in <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">the other post</a>.</li>
<li id="5b">To find $J_f(g(x))$, it is required to have $f$ in terms of each coordinate of $g(x)$ in order to be able to find all the first-order partial derivatives. If $f$ and $g$ are given separately as two functions like $f(z)$ and $g(x)$, to find $J_f(g(x))$ it suffice to find the jacobian of $f$ with respect to $z$ then substitute coordinates of $g(x)$ in as the coordinates of $z$. Precisely, we have $J_f(z) \circ g(x)$.</li>
<li id="6b">When $h(x)$ is given as a function of $x$, then we are back to the case where Jacobian matrix can be found as the way we found it for a vector-valued fucntion in <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">the other post</a>.</li>
</ul>
<p>In what follows, I will walk you through what I discussed for the above case. Suppose we are given</p>
<p>$f\left(u,v\right)=u^{2}+3v^{2}$,</p>
<p>$g\left(x,y\right)=\begin{bmatrix} e^{x}\cos y  \\ e^{x}\sin y \end{bmatrix} $,</p>
<p>and the goal is finding the Jacobian matrix of the composite function.  Also, let the composite function be $h$.</p>
<p><strong>Question: </strong>$h$ is the composite function of $f$ and $g$ or $g$ and $f$?</p>
<p><strong>Answer: </strong>To clarify the above question, we are required to think of $f$ and $g$ as two mappings where $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ and $g: \mathbb{R}^2 \rightarrow \mathbb{R}^2$.</p>
<p>Suppose $h = f \circ g$.</p>
<p>Since $g$ is the last function in the composite function, variables of $h$ are going to be vaibales of $g$. Therefore, $J_h(x, y) = J_f(g(u,v))J_g(x, y)$. <a href="#1b">According to the first and second bullet</a>, $J_f(g(u,v))$ is a $1 \times 2$ matrix and $J_g(x, y)$ is a $2 \times 2$ matrix. Hence, the result is a $1 \times 2$ matrix. However, if we flip $f$ and $g$, and consider $h$ as $h=g \circ f$ and we should have</p>
<p>$$J_h(u, v) \stackrel{?}{=} J_g(f(u,v))J_f(u, y)$$</p>
<p>It is easy to see <a href="#1b">from the first bullet</a> that we have a $1 \times 2$ for $J_f(u, v)$ matrix but $J_g(f(x,y))$ is not defind since one should take the derivatives of all coordinates of $g$ with respect to all varibales of $g$ which are now coordinates of $f$; but $f$ is a scalar-valued function and does not output the same dimension as $g$ input needs. In effect, $h=g \circ f$ is not defined. Hence, by compostion function, we mean $h = f \circ g$.</p>
<p>Let's first compute $J_g(x, y)$ where $g$ is a function of $x,y$ and it is a vector-valued function so</p>
<p>$$<br> J_g(x, y) = \begin{bmatrix}<br>\frac{\partial g_1}{\partial x} &amp; \frac{\partial g_1}{\partial y} \\<br>\frac{\partial g_2}{\partial x} &amp; \frac{\partial g_2}{\partial y} <br> \end{bmatrix}=<br>\begin{bmatrix}<br>e^x\cos y &amp; -e^x\sin y \\ e^x\sin y &amp; e^x\cos y<br>\end{bmatrix}$$  </p>
<p>Now to compute $J_f(g(u,v))$ <a href="#5b">using the fifth bullet</a> which yields<br>$$<br>\begin{align}<br>J_f(g(u,v)) &amp;= J_f(z = (u,v))\circ g(x,y)\\<br> &amp;= <br>\begin{bmatrix}<br>\frac{\partial f}{\partial u} &amp; \frac{\partial f}{\partial v}<br> \end{bmatrix} \circ g(x,y) \\<br>&amp;=\begin{bmatrix}<br>2u &amp; 6v<br>\end{bmatrix}\circ g(x,y)\\<br>&amp;=\begin{bmatrix}<br>2u &amp; 6v<br>\end{bmatrix}\circ<br>\begin{bmatrix}<br>e^{x}\cos y \\ e^{x}\sin y<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix} 2e^{x}\cos y &amp; 6e^{x}\sin y \end{bmatrix}<br>\end{align}<br>$$<br><br>Finally,</p>
<p>$$<br>\begin{align}<br>J_{h}(x,y) &amp;= J_{f \circ g}(x,y) = J_f(g(u,v)) J_g(x, y)\\<br>&amp;=<br>\begin{bmatrix} 2e^{x}\cos y &amp; 6e^{x}\sin y \end{bmatrix}<br>\begin{bmatrix}<br>e^x\cos y &amp; -e^x\sin y \\ e^x\sin y &amp; e^x\cos y<br>\end{bmatrix}\\<br>&amp;= <br>\begin{bmatrix}<br>2e^{2x}\cos^2y + 6e^{2x}\sin^2y &amp; -2e^{2x}\cos y \sin y + 6e^{2x}\sin y\cos y<br>\end{bmatrix} \\<br>&amp;= <br>2e^{2x}<br>\begin{bmatrix}<br>1 + 2\sin^2y &amp; 2\sin y\cos y <br>\end{bmatrix}<br>\end{align}\\<br>$$</p>
<p> <strong>Conclusion: </strong>We went through all the steps of finding the Jacobian matrix of a composite function consists of two functions. It is one step left to apply all these result to the lsos function of a neural network.</p>
<p> </p>
            ]]>
        </content>
    </entry>
</feed>
