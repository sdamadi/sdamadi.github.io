<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Linear neural network to creat $f(x)&#x3D; W^{\top}x$ - Saeed Damadi</title><meta name="robots" content="index, follow"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://sdamadi.github.io/how-to-use-linear-nn-to.html"><link rel="amphtml" href="https://sdamadi.github.io/amp/how-to-use-linear-nn-to.html"><link type="application/atom+xml" rel="alternate" href="https://sdamadi.github.io/feed.xml"><meta property="og:title" content="Linear neural network to creat $f(x)= W^{\top}x$ "><meta property="og:site_name" content="My blog - Saeed Damadi"><meta property="og:description" content="Goal:In this post we are going to go through the very first step that help us to suppress algebraic notations for representing the function calculating by a fully connected neural network. Following this step helps you to master shorthand notations to express operations happening in&hellip;"><meta property="og:url" content="https://sdamadi.github.io/how-to-use-linear-nn-to.html"><meta property="og:type" content="article"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin><link href="https://fonts.googleapis.com/css?family=Roboto:400,700|Roboto+Condensed:400,700&amp;subset=latin-ext" rel="stylesheet"><link rel="stylesheet" href="https://sdamadi.github.io/assets/css/style.css?v=aa41620c4f80dc7a8874270e3bdddd5b"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://sdamadi.github.io/how-to-use-linear-nn-to.html"},"headline":"Linear neural network to creat $f(x)= W^{\\top}x$ ","datePublished":"2019-12-25T17:57","dateModified":"2019-12-25T21:36","description":"Goal:In this post we are going to go through the very first step that help us to suppress algebraic notations for representing the function calculating by a fully connected neural network. Following this step helps you to master shorthand notations to express operations happening in&hellip;","author":{"@type":"Person","name":"Saeed"},"publisher":{"@type":"Organization","name":"Saeed"}}</script><script async src="https://sdamadi.github.io/assets/js/lazysizes.min.js?v=dc4b666bb3324aea4ead22e26059c761"></script><style>.top__search [type=search] {
						background-image: url(https://sdamadi.github.io/assets/svg/search.svg);
				    }</style><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script></head><body><div class="container"><header class="js-top is-sticky"><a class="logo" href="https://sdamadi.github.io">Saeed Damadi</a><div class="top"><nav class="navbar js-navbar"><button class="navbar__toggle js-navbar__toggle">Menu</button><ul class="navbar__menu"></ul></nav><div class="top__search"><form action="https://sdamadi.github.io/search.html" class="search"><input type="search" name="q" placeholder="search..."></form></div></div></header><main><article class="post"><div class="hero"><header class="hero__text"><h1>Linear neural network to creat $f(x)&#x3D; W^{\top}x$</h1><p class="post__meta"><time datetime="2019-12-25T17:57">December 25, 2019 </time>By <a href="https://sdamadi.github.io/authors/saeed/" rel="author" title="Saeed">Saeed</a></p></header></div><div class="post__entry"><h3>Goal:</h3><p>In this post we are going to go through the very first step that help us to suppress algebraic notations for representing the function calculating by a fully connected neural network. Following this step helps you to master shorthand notations to express operations happening in the neural network.</p><h3>A linear function in the form of $f(x)= W^{\top}x$</h3><p><a href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html">In the previous post</a> we discussed how to find the Jacobian matrix of $f(x)= Ax$ where $A \in \mathbb{R}^{m \times n}$ and $x = [x_1, x_2, \cdots, x_n]^{\top}$. Also, I showed how to represent $f(x)$ in terms of rows of $A$. However, here, we are dealing with $W^{\top}$ in place of $A$ where $W \in \mathbb{R}^{n \times m}$  which requires us to do some work. In order to show how we can build this function using a linear neural network. Suppose $y= f(x)$. Therefore, each coordinate of $f(x)$ equals to each coordinate of a $y$. That is</p><p>$$<br>\begin{align}<br>y &amp;=<br>\begin{bmatrix}<br>y_1\\<br>y_2\\<br>\vdots\\<br>y_n<br>\end{bmatrix} =<br>f(x)=W^{\top}x\\<br>&amp;\stackrel{(1)}{=}<br>\begin{bmatrix}<br>W_{\bullet1} &amp; W_{\bullet2} &amp; \cdots &amp; W_{\bullet m}<br>\end{bmatrix}^{\top}x\\<br>&amp;\stackrel{(2)}{=}<br>\begin{bmatrix}<br>W_{\bullet 1}\\<br>W_{\bullet 2}\\\\<br>\vdots\\<br>W_{\bullet m}\\<br>\end{bmatrix}x<br>\stackrel{(3)}{=}<br>\begin{bmatrix}<br>W_{\bullet 1}x\\<br>W_{\bullet 2}x\\\\<br>\vdots\\<br>W_{\bullet m}x\\<br>\end{bmatrix}\\<br>&amp;\stackrel{(4)}{=}<br>\begin{bmatrix}<br>w_{11}x_1 + w_{21}x_2 + \cdots + w_{n1}x_n\\<br>w_{12}x_1 + w_{22}x_2 + \cdots + w_{n2}x_n\\<br>\vdots\\<br>w_{1m}x_1 + w_{2m}x_2 + \cdots + w_{nm}x_n\\<br>\end{bmatrix}<br>\end{align}<br>$$</p><p>where $W_{i\bullet}$ is the $i$-th row of $W$ and $W_{\bullet j}$ is the $j$-th column of $W$. Also,</p><ul><li>(1) is the representation of $W$ columnwise with $m$ columns($W \in \mathbb{R}^{n \times m}$)</li><li>(2) is the transpose of $w$ so every column becomes a row</li><li>(3) and (4) are because of matrix multiplication rule</li></ul><p>I had promissed to represent $f(x)= W^{\top}x$ using neural network. To that end, look at the following simple neural network where we have only input and output layers. Input layer has $2$ elements and output layer has $3$ leyers. Each input gets multiplied by its corresponding weight and adds to the other product in order to build each element of $y$.</p><figure class="post__image post__image--center"><img class="lazyload" src="https://sdamadi.github.io/media/posts/7/responsive/Simplest-neural-network-without-weights-xs.png" data-sizes="auto" data-srcset="https://sdamadi.github.io/media/posts/7/responsive/Simplest-neural-network-without-weights-xs.png 300w ,https://sdamadi.github.io/media/posts/7/responsive/Simplest-neural-network-without-weights-sm.png 480w ,https://sdamadi.github.io/media/posts/7/responsive/Simplest-neural-network-without-weights-md.png 768w" alt="Simlest neural network" width="300" height="231"><figcaption>A simple inear neural network without weight assignments</figcaption></figure><p>If I annotate every arrow using a weight $w$ whose first index is associated with input and second one with the output we have the following network.</p><figure class="post__image post__image--center"><img class="lazyload" src="https://sdamadi.github.io/media/posts/7/responsive/NN-With-weights-xs.png" data-sizes="auto" data-srcset="https://sdamadi.github.io/media/posts/7/responsive/NN-With-weights-xs.png 300w ,https://sdamadi.github.io/media/posts/7/responsive/NN-With-weights-sm.png 480w ,https://sdamadi.github.io/media/posts/7/responsive/NN-With-weights-md.png 768w" alt="With weights" width="300" height="227"><figcaption>A simple inear neural network with weight assignments</figcaption></figure><p> If you carefully look, we can write each element of $y$ using linear combination of $x$'s element and $w$'s as the following</p><p>$$<br>\begin{align}<br>y &amp;=<br>\begin{bmatrix}<br>y_1\\<br>y_2\\<br>y_3\\<br>\end{bmatrix} =<br>\begin{bmatrix}<br>W_{11}x_1 + W_{21}x_2 \\<br>W_{12}x_1 + W_{22}x_2 n\\<br>W_{13}x_1 + W_{23}x_2 \\<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix}<br>W_{11} &amp; W_{21}\\<br>W_{12} &amp; W_{22}\\<br>W_{13} &amp; W_{23} \\<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1\\<br>x_2\\<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix}<br>W_{11} &amp; W_{12} &amp; W_{13}\\<br>W_{21} &amp; W_{22} &amp; W_{23}\\<br>\end{bmatrix}^{\top}<br>\begin{bmatrix}<br>x_1\\<br>x_2\\<br>\end{bmatrix}<br>=W^{\top}x = f(x)<br>\end{align}<br>$$<br><br></p><p>Hence, from now on, I will assign $W$ between each two layers because we can think of them as input($x$) and output($y$) where output formula is $y = W^{\top}x$.</p><figure class="post__image post__image--center"><img class="lazyload" src="https://sdamadi.github.io/media/posts/7/responsive/Single-leyer-Single-Weight-xs.png" data-sizes="auto" data-srcset="https://sdamadi.github.io/media/posts/7/responsive/Single-leyer-Single-Weight-xs.png 300w ,https://sdamadi.github.io/media/posts/7/responsive/Single-leyer-Single-Weight-sm.png 480w ,https://sdamadi.github.io/media/posts/7/responsive/Single-leyer-Single-Weight-md.png 768w" alt="One weight" width="300" height="659"><figcaption>Shorthand notation</figcaption></figure><p>In the next post I will add another element to our function which is called biass and because of that our function would be $W^{\top}x + b$.</p><p class="post__last-updated">This article was updated on December 25, 2019</p></div><aside class="post__share"></aside><footer class="post__footer"><div class="post__bio"><h3><a href="https://sdamadi.github.io/authors/saeed/" class="inverse" title="Saeed">Saeed</a></h3></div><nav class="post__nav"><div class="post__nav__prev">Previous Post<h5><a href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html" class="inverse" rel="prev">Compute the gradient of a neural network</a></h5></div><div class="post__nav__next">Next Post<h5><a href="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html" class="inverse" rel="next">Nonlinear neural network to creat $F(x)&#x3D;f( W^{\top}x+b)$</a></h5></div></nav><div class="post__related"><h3>Related posts</h3><div class="post__related__wrap"><figure><figcaption><h4><a href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html" class="inverse">What is Jacobian matrix of $f(x)&#x3D; Ax$?</a></h4><time datetime="2019-12-25T14:54">December 25, 2019</time></figcaption></figure><figure><figcaption><h4><a href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html" class="inverse">Compute the gradient of a neural network</a></h4><time datetime="2019-12-25T14:57">December 25, 2019</time></figcaption></figure><figure><figcaption><h4><a href="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html" class="inverse">Nonlinear neural network to creat $F(x)&#x3D;f( W^{\top}x+b)$</a></h4><time datetime="2019-12-25T20:12">December 25, 2019</time></figcaption></figure></div></div></footer></article></main><footer class="footer"><div class="footer__copyright">Powered by Publii</div></footer></div><script defer="defer" src="https://sdamadi.github.io/assets/js/jquery-3.2.1.slim.min.js?v=f307ecc7f6353949f03c06ffa70652a2"></script><script defer="defer" src="https://sdamadi.github.io/assets/js/scripts.min.js?v=f37607af05f83050cdcc37ea18bb5ee1"></script></body></html>