<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Nonlinear neural network to creat $F(x)&#x3D;f( W^{\top}x+b)$ - Saeed Damadi</title><meta name="robots" content="index, follow"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"><link rel="amphtml" href="https://sdamadi.github.io/amp/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"><link type="application/atom+xml" rel="alternate" href="https://sdamadi.github.io/feed.xml"><meta property="og:title" content="Nonlinear neural network to creat $F(x)=f( W^{\top}x+b)$"><meta property="og:site_name" content="My blog - Saeed Damadi"><meta property="og:description" content="The next step towards finding loss function of a neural network is to extend the results we found here to add a bias to the linear function, i.e., creating $W^{\top}x+b$ where $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, and $W \in \mathbb{R}^{n \times m}$. This can be done easily because&hellip;"><meta property="og:url" content="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"><meta property="og:type" content="article"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin><link href="https://fonts.googleapis.com/css?family=Roboto:400,700|Roboto+Condensed:400,700&amp;subset=latin-ext" rel="stylesheet"><link rel="stylesheet" href="https://sdamadi.github.io/assets/css/style.css?v=aa41620c4f80dc7a8874270e3bdddd5b"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"},"headline":"Nonlinear neural network to creat $F(x)=f( W^{\\top}x+b)$","datePublished":"2019-12-25T20:12","dateModified":"2019-12-26T03:02","description":"The next step towards finding loss function of a neural network is to extend the results we found here to add a bias to the linear function, i.e., creating $W^{\\top}x+b$ where $x \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}^m$, and $W \\in \\mathbb{R}^{n \\times m}$. This can be done easily because&hellip;","author":{"@type":"Person","name":"Saeed"},"publisher":{"@type":"Organization","name":"Saeed"}}</script><script async src="https://sdamadi.github.io/assets/js/lazysizes.min.js?v=dc4b666bb3324aea4ead22e26059c761"></script><style>.top__search [type=search] {
						background-image: url(https://sdamadi.github.io/assets/svg/search.svg);
				    }</style><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script></head><body><div class="container"><header class="js-top is-sticky"><a class="logo" href="https://sdamadi.github.io">Saeed Damadi</a><div class="top"><nav class="navbar js-navbar"><button class="navbar__toggle js-navbar__toggle">Menu</button><ul class="navbar__menu"></ul></nav><div class="top__search"><form action="https://sdamadi.github.io/search.html" class="search"><input type="search" name="q" placeholder="search..."></form></div></div></header><main><article class="post"><div class="hero"><header class="hero__text"><h1>Nonlinear neural network to creat $F(x)&#x3D;f( W^{\top}x+b)$</h1><p class="post__meta"><time datetime="2019-12-25T20:12">December 25, 2019 </time>By <a href="https://sdamadi.github.io/authors/saeed/" rel="author" title="Saeed">Saeed</a></p></header></div><div class="post__entry"><p>The next step towards finding loss function of a neural network is to extend the results we found <a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html">here</a> to add a bias to the linear function, i.e., creating $W^{\top}x+b$ where $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, and $W \in \mathbb{R}^{n \times m}$. This can be done easily because we already have the part of a network that generates $W^{\top}x$. It suffice to add a constant vector to the previous result. Therefore, we have the follwing for the case where $n = 2$ and $m = 3$</p><figure class="post__image post__image--center"><img class="lazyload" src="https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-xs.png" data-sizes="auto" data-srcset="https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-xs.png 300w ,https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-sm.png 480w ,https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-md.png 768w" alt="" width="300" height="628"><figcaption>Adding bias to the neural network</figcaption></figure><p> If we pass the above quantity to the function $f$, we get $F(x)$.</p><h3> Jacobian matrix of $F(x)=f( W^{\top}x+b)$</h3><p>A relevant question at this moment is how we can find the Jacobian of $F(x)$ using what we learned <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">in this post</a> and <a href="https://sdamadi.github.io/jacobian-of-a-composition-function.html">the other one</a>s. We know Jacobian matrix of a composite function is the product of Jacobians so </p><p>$$<br>J_F(x) = J_{f}(W^{\top}x+b)J_{W^{\top}x+b}(x) = J_{f}(h)\circ (W^{\top}x+b) J_{W^{\top}x+b}(x)<br>$$</p><p>where $h = W^{\top}x+b$. Note that Jacobian of $f$ is being taken with respect to $h$. In neural networks $f$ is a scalar function which is applied to each element of $h$ separately so we have the following</p><p>$$<br>f(h) =<br>\begin{bmatrix}<br>f(h_1)\\<br>f(h_2)\\<br>\vdots\\<br>f(h_m)<br>\end{bmatrix}<br>$$</p><p>According to what we have discussed <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">here</a>, $J_f(h)$ is all first-order partial derivatives of $f(h)$ as the following</p><p>$$<br>J_f(h)= \begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; \frac{\partial f(h_1)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_1)}{\partial h_m}\\<br>\frac{\partial f(h_2)}{\partial h_1} &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_2)}{\partial h_m}\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>\frac{\partial f(h_m)}{\partial h_1} &amp; \frac{\partial f(h_m)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}<br>$$</p><p>It is remarkable that because of the structure that we define for the neural network, we get a diagonal matrix for the Jacobian of $J_f(h)$ where each element of diagonal is the same. However, we are after $J_{f}(h)\circ (W^{\top}x+b)$ which says whereever we have one coordinate of $h$, i.e., $h_i$ where $i = 1, 2, \cdots, m$, substitute the coresponding coordinate of $W^{\top}x+b$.</p><p>$$<br>\begin{align}<br>J_f(h)\circ (W^{\top}x+b)<br>&amp;=<br>\begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}\circ (W^{\top}x+b)\\<br>&amp;=<br>\begin{bmatrix}<br>f'(h_1) &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; f'(h_2)&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; f'(h_m)<br>\end{bmatrix}<br>=\phi'_f(h)<br>\end{align}<br>$$</p><p>where $\phi'_f(h)$ is defined as a digonal matrix whose diagonal is the derivative of $f$ at the corresponding coordinates of $h$. </p><p><a href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html">From this post</a> we know Jacobian matrix of $J_{W^{\top}x+b}(x)$ is just $W^{\top}$.</p><p>Therefore, $$<br>J_F(x) = J_{f}(h)\circ (W^{\top}x+b) J_{W^{\top}x+b}(x) = \phi'_f(h)W^{\top}<br>$$</p><p>To recap what we have done so far take a look at suppressed network to see how by looking at the structure of the network we can find the Jacobian matrix.</p><p> </p><p>We are going to put different layers to gether and use the above result to find the Jacobian matrix of a neural network.</p><p> </p><p class="post__last-updated">This article was updated on December 26, 2019</p></div><aside class="post__share"></aside><footer class="post__footer"><div class="post__bio"><h3><a href="https://sdamadi.github.io/authors/saeed/" class="inverse" title="Saeed">Saeed</a></h3></div><nav class="post__nav"><div class="post__nav__prev">Previous Post<h5><a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html" class="inverse" rel="prev">Linear neural network to creat $f(x)&#x3D; W^{\top}x$</a></h5></div></nav><div class="post__related"><h3>Related posts</h3><div class="post__related__wrap"><figure><figcaption><h4><a href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html" class="inverse">Compute the gradient of a neural network</a></h4><time datetime="2019-12-25T14:57">December 25, 2019</time></figcaption></figure><figure><figcaption><h4><a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html" class="inverse">Linear neural network to creat $f(x)&#x3D; W^{\top}x$</a></h4><time datetime="2019-12-25T17:57">December 25, 2019</time></figcaption></figure></div></div></footer></article></main><footer class="footer"><div class="footer__copyright">Powered by Publii</div></footer></div><script defer="defer" src="https://sdamadi.github.io/assets/js/jquery-3.2.1.slim.min.js?v=f307ecc7f6353949f03c06ffa70652a2"></script><script defer="defer" src="https://sdamadi.github.io/assets/js/scripts.min.js?v=f37607af05f83050cdcc37ea18bb5ee1"></script></body></html>