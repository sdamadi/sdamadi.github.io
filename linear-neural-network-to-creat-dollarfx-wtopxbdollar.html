<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Creating nonlinear neural network and finding the Jacobian matrix of its funciton - Saeed Damadi</title><meta name="robots" content="index, follow"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"><link rel="amphtml" href="https://sdamadi.github.io/amp/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"><link type="application/atom+xml" rel="alternate" href="https://sdamadi.github.io/feed.xml"><meta property="og:title" content="Creating nonlinear neural network and finding the Jacobian matrix of its funciton"><meta property="og:site_name" content="My blog - Saeed Damadi"><meta property="og:description" content="The next step towards finding loss function of a neural network is to extend the results we found here to add a bias to the linear function, i.e., creating $W^{\top}x+b$ where $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, and $W \in \mathbb{R}^{n \times m}$. This can be done easily because&hellip;"><meta property="og:url" content="https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"><meta property="og:type" content="article"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin><link href="https://fonts.googleapis.com/css?family=Roboto:400,700|Roboto+Condensed:400,700&amp;subset=latin-ext" rel="stylesheet"><link rel="stylesheet" href="https://sdamadi.github.io/assets/css/style.css?v=aa41620c4f80dc7a8874270e3bdddd5b"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://sdamadi.github.io/linear-neural-network-to-creat-dollarfx-wtopxbdollar.html"},"headline":"Creating nonlinear neural network and finding the Jacobian matrix of its funciton","datePublished":"2019-12-25T20:12","dateModified":"2019-12-28T20:28","description":"The next step towards finding loss function of a neural network is to extend the results we found here to add a bias to the linear function, i.e., creating $W^{\\top}x+b$ where $x \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}^m$, and $W \\in \\mathbb{R}^{n \\times m}$. This can be done easily because&hellip;","author":{"@type":"Person","name":"Saeed"},"publisher":{"@type":"Organization","name":"Saeed"}}</script><script async src="https://sdamadi.github.io/assets/js/lazysizes.min.js?v=dc4b666bb3324aea4ead22e26059c761"></script><style>.top__search [type=search] {
						background-image: url(https://sdamadi.github.io/assets/svg/search.svg);
				    }</style><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script></head><body><div class="container"><header class="js-top is-sticky"><a class="logo" href="https://sdamadi.github.io">Saeed Damadi</a><div class="top"><nav class="navbar js-navbar"><button class="navbar__toggle js-navbar__toggle">Menu</button><ul class="navbar__menu"></ul></nav><div class="top__search"><form action="https://sdamadi.github.io/search.html" class="search"><input type="search" name="q" placeholder="search..."></form></div></div></header><main><article class="post"><div class="hero"><header class="hero__text"><h1>Creating nonlinear neural network and finding the Jacobian matrix of its funciton</h1><p class="post__meta"><time datetime="2019-12-25T20:12">December 25, 2019 </time>By <a href="https://sdamadi.github.io/authors/saeed/" rel="author" title="Saeed">Saeed</a></p></header></div><div class="post__entry"><p>The next step towards finding loss function of a neural network is to extend the results we found <a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html">here</a> to add a bias to the linear function, i.e., creating $W^{\top}x+b$ where $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, and $W \in \mathbb{R}^{n \times m}$. This can be done easily because we already have the part of a network that generates $W^{\top}x$. It suffice to add a constant vector to the previous result. Therefore, we have the follwing for the case where $n = 2$ and $m = 3$</p><figure class="post__image post__image--center"><img class="lazyload" src="https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-xs.png" data-sizes="auto" data-srcset="https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-xs.png 300w ,https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-sm.png 480w ,https://sdamadi.github.io/media/posts/8/responsive/Neural-Network-with-bias-md.png 768w" alt="" width="300" height="628"><figcaption>Adding bias to the neural network</figcaption></figure><p>Now to create a nonlinear function $F(x)=f( W^{\top}x+b)$, we need to pass in the above quantity to the function $f$, which produces $F(x)$.</p><h3> Jacobian matrix of $F(x)=f( W^{\top}x+b)$</h3><p>A relevant question at this moment is how we can find the Jacobian of $F(x)$ using what we learned <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">in this post</a> and <a href="https://sdamadi.github.io/jacobian-of-a-composition-function.html">the other one</a>. We know Jacobian matrix of a composite function is the product of Jacobians so </p><p>$$<br>J_F(x) = J_{f}(W^{\top}x+b)J_{W^{\top}x+b}(x) = J_{f}(h)\circ (W^{\top}x+b) J_{W^{\top}x+b}(x)<br>$$</p><p>where $h = W^{\top}x+b$. Note that Jacobian of $f$ is being taken with respect to $h$. In neural networks $f$ is a scalar function which is applied to each element of $h$ separately so we have the following</p><p>$$<br>f(h) =<br>\begin{bmatrix}<br>f(h_1)\\<br>f(h_2)\\<br>\vdots\\<br>f(h_m)<br>\end{bmatrix}<br>$$</p><p>According to what we have discussed <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">here</a>, $J_f(h)$ is all first-order partial derivatives of $f(h)$ as the following</p><p>$$<br>J_f(h)= \begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; \frac{\partial f(h_1)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_1)}{\partial h_m}\\<br>\frac{\partial f(h_2)}{\partial h_1} &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_2)}{\partial h_m}\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>\frac{\partial f(h_m)}{\partial h_1} &amp; \frac{\partial f(h_m)}{\partial h_2}&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}<br>$$</p><p>It is remarkable that because of the structure that we define for the neural network, we get a diagonal matrix for the Jacobian of $J_f(h)$ where each element of its diagonal is functionally the same but is being represented by different variables. However, we are after $J_{f}(h)\circ (W^{\top}x+b)$ which says wherever we have one coordinate of $h$, i.e., $h_i$ where $i = 1, 2, \cdots, m$, substitute the coresponding coordinate of $W^{\top}x+b$.</p><p>$$<br>\begin{align}<br>J_f(h)\circ (W^{\top}x+b)<br>&amp;=<br>\begin{bmatrix}<br>\frac{\partial f(h_1)}{\partial h_1} &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; \frac{\partial f(h_2)}{\partial h_2}&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; \frac{\partial f(h_m)}{\partial h_m}<br>\end{bmatrix}\circ (W^{\top}x+b)\\<br>&amp;=<br>\begin{bmatrix}<br>f'(h_1) &amp; 0&amp; \cdots&amp; 0\\<br>0 &amp; f'(h_2)&amp; \cdots&amp; 0\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>0 &amp; 0&amp; \cdots&amp; f'(h_m)<br>\end{bmatrix}<br>=\phi'_f(h)<br>\end{align}<br>$$</p><p>where $\phi'_f(h)$ is defined as a digonal matrix whose diagonal is the derivative of $f$ at the corresponding coordinates of $h$. </p><p><a href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html">From this post</a> we know Jacobian matrix of $J_{W^{\top}x+b}(x)$ is just $W^{\top}$.</p><p>Therefore, $$<br>J_F(x) = J_{f}(h)\circ (W^{\top}x+b) J_{W^{\top}x+b}(x) = \phi'_f(h)W^{\top}<br>$$</p><p>In the following I am going to elaborate on a neural network that has more layer than input and output and derive its Jacobian matrix. Consifer the following network</p><figure class="post__image post__image--center"><img class="lazyload" src="https://sdamadi.github.io/media/posts/8/responsive/two-layers-xs.png" data-sizes="auto" data-srcset="https://sdamadi.github.io/media/posts/8/responsive/two-layers-xs.png 300w ,https://sdamadi.github.io/media/posts/8/responsive/two-layers-sm.png 480w ,https://sdamadi.github.io/media/posts/8/responsive/two-layers-md.png 768w" width="600" height="876"><figcaption>A nonlinear neural network with more than input and output layers </figcaption></figure><p> Using the above we can write </p><p>$$<br>\begin{align}<br>F(x)&amp;=a_2\\<br>&amp;= f_2(w_2^{\top}a_1+b_2)\\<br>F(x)&amp;=<br>f_2(w_2^{\top}f_1(w_1^{\top}x+b_1)+b_2)\\<br>F(x)&amp;=f_2\circ(w_2^{\top}u+b_2)\circ f_1( w_1^{\top}x+b_1) <br>\end{align}<br>$$</p><p>Since $a_1 = w_1^{\top}x+b_1$.</p><p>In order to find the Jacobian matrix with respect to the input we have to use the multiplication rule that we derived here. Therefore,</p><p>$$<br>\begin{align}<br>J_F(x)&amp;= J_{f_2}(w_2^{\top} a_1 + b_2) J_{f_1}(w_1^{\top} x + b_1)\\<br>&amp;= \phi'_{f_2}(a_2)w_2^{\top}\phi'_{f_1}(a_1)w_1^{\top}<br>\end{align}<br>$$</p><p>If you associate the above formula with the figure, you can conclude that no matter how many layers we have, the same pattern would repeat for them.</p><p>To recap what we have done so far, we can claim that by looking at the suppressed network, we can find the Jacobian matrix. </p><p class="post__last-updated">This article was updated on December 28, 2019</p></div><aside class="post__share"></aside><footer class="post__footer"><div class="post__bio"><h3><a href="https://sdamadi.github.io/authors/saeed/" class="inverse" title="Saeed">Saeed</a></h3></div><nav class="post__nav"><div class="post__nav__prev">Previous Post<h5><a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html" class="inverse" rel="prev">Linear neural network to creat $f(x)&#x3D; W^{\top}x$</a></h5></div><div class="post__nav__next">Next Post<h5><a href="https://sdamadi.github.io/stochastic-gradient-descent-versus-batch-gradient-descent.html" class="inverse" rel="next">Stochastic gradient descent versus batch gradient descent</a></h5></div></nav><div class="post__related"><h3>Related posts</h3><div class="post__related__wrap"><figure><figcaption><h4><a href="https://sdamadi.github.io/jacobian-of-a-composition-function.html" class="inverse">Jacobian matrix of a composite function</a></h4><time datetime="2019-12-23T17:58">December 23, 2019</time></figcaption></figure><figure><figcaption><h4><a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html" class="inverse">What is Jacobian matrix and why do we need it?</a></h4><time datetime="2019-12-23T19:02">December 23, 2019</time></figcaption></figure><figure><figcaption><h4><a href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html" class="inverse">What is Jacobian matrix of $f(x)&#x3D; Ax$?</a></h4><time datetime="2019-12-25T14:54">December 25, 2019</time></figcaption></figure></div></div></footer></article></main><footer class="footer"><div class="footer__copyright">Powered by Publii</div></footer></div><script defer="defer" src="https://sdamadi.github.io/assets/js/jquery-3.2.1.slim.min.js?v=f307ecc7f6353949f03c06ffa70652a2"></script><script defer="defer" src="https://sdamadi.github.io/assets/js/scripts.min.js?v=f37607af05f83050cdcc37ea18bb5ee1"></script></body></html>