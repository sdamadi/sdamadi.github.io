<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>What is Jacobian matrix of $f(x)&#x3D; Ax$? - S. M. Saeed Damadi</title><meta name="robots" content="index, follow"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html"><link rel="amphtml" href="https://sdamadi.github.io/amp/what-is-jacobian-matrix-of-dollarfx-axdollar.html"><link type="application/atom+xml" rel="alternate" href="https://sdamadi.github.io/feed.xml"><meta property="og:title" content="What is Jacobian matrix of $f(x)= Ax$?"><meta property="og:site_name" content="My blog - S. M. Saeed Damadi"><meta property="og:description" content="In this short post we are going to find the Jacobian matrix&hellip;"><meta property="og:url" content="https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html"><meta property="og:type" content="article"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin><link href="https://fonts.googleapis.com/css?family=Heebo:400,500|Playfair+Display:400" rel="stylesheet"><link rel="stylesheet" href="https://sdamadi.github.io/assets/css/style.css?v=ea52f977eeda8cd9d2cc86a064813e8e"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://sdamadi.github.io/what-is-jacobian-matrix-of-dollarfx-axdollar.html"},"headline":"What is Jacobian matrix of $f(x)= Ax$?","datePublished":"2019-12-25T14:54","dateModified":"2019-12-25T17:55","description":"In this short post we are going to find the Jacobian matrix&hellip;","author":{"@type":"Person","name":"Saeed"},"publisher":{"@type":"Organization","name":"Saeed"}}</script><script async src="https://sdamadi.github.io/assets/js/lazysizes.min.js?v=dc4b666bb3324aea4ead22e26059c761"></script><style></style><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script></head><body><div class="container"><header class="js-top"><a class="logo" href="https://sdamadi.github.io">S. M. Saeed Damadi</a><div class="top"><nav class="navbar js-navbar"><button class="navbar__toggle js-navbar__toggle" aria-expanded="false">Menu</button><ul class="navbar__menu"></ul></nav><div class="top__search"><form action="https://sdamadi.github.io/search.html" class="search"><input type="search" name="q" placeholder="search..."></form></div></div></header><main><article class="post u-wrapper"><header class="hero"><p class="post__meta">By <a href="https://sdamadi.github.io/authors/saeed/" rel="author" title="Saeed">Saeed</a> Published on <time datetime="2019-12-25T14:54">December 25, 2019</time></p><h1 class="post__title">What is Jacobian matrix of $f(x)&#x3D; Ax$?</h1></header><div class="post__entry"><p>In this short post we are going to find the Jacobian matrix of $f(x)= Ax$ where $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, $x \in \mathbb{R}^n$, and $A \in \mathbb{R}^{m \times n}$.</p><h3>Why this function is so important?</h3><ol><li>It is a linear function</li><li>It appers in neural networks when we want to find <a href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html">the loss function</a></li><li>It generalize $f(x)= ax$ where $x$, and $a$ are scalars.</li></ol><p><a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html#mcetoc_1dsqmf85h2">As I explained here</a>, in order to find the Jacobian matrix we need a vector-valued function which we have but one should be able to represent each coordinte of $f$ as a function so we do the following:</p><p>$$<br>\begin{align}<br>f(x)&amp;= Ax =<br>\begin{bmatrix}<br>A_{1\bullet}\\<br>A_{2\bullet}\\<br>\vdots\\<br>A_{m\bullet}\\<br>\end{bmatrix}<br>x<br>=<br>\begin{bmatrix}<br>A_{1\bullet}x\\<br>A_{2\bullet}x\\<br>\vdots\\<br>A_{m\bullet}x\\<br>\end{bmatrix}=<br>\begin{bmatrix}<br>f_1(x)\\<br>f_2(x)\\<br>\vdots\\<br>f_n(x)\\<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix}<br>a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n\\<br>a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n\\<br>\vdots\\<br>a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_nx\\<br>\end{bmatrix}\\<br>\end{align}<br>$$</p><p>where $A_{i\bullet}$ is the $i$-th row of $A$ and $x = [x_1, x_2, \cdots, x_n]^{\top}$.<br>Hence according to what we discussed <a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html">in the other post</a>,<br>$$<br>J_f(x) = \begin{bmatrix}<br>a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\<br>a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots\\<br>a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\<br>\end{bmatrix}<br>=A<br>$$</p><p><strong>Note</strong>: It does not make any differences if we add a constant vector $b$ to $Ax$, i.e., $f(x) = Ax + b$.</p></div><footer class="post__footer"><div class="post__last-updated">This article was updated on December 25, 2019</div><div class="post__footer__col"><div class="post__share"></div></div><nav class="post__nav"><div class="post__nav__prev">Previous Post<h5><a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html" class="inverse" rel="prev">What is Jacobian matrix and why do we need it?</a></h5></div><div class="post__nav__next">Next Post<h5><a href="https://sdamadi.github.io/compute-the-gradient-of-neural-network-loss-function.html" class="inverse" rel="next">Compute the gradient of a neural network</a></h5></div></nav><div class="post__bio"><div><h3><a href="https://sdamadi.github.io/authors/saeed/" class="inverse" title="Saeed">Saeed</a></h3></div></div><div class="post__related"><h3 class="u-h5">Related posts</h3><div class="post__related__wrap"><figure><figcaption><h4><a href="https://sdamadi.github.io/jacobian-of-a-composition-function.html" class="inverse">Jacobian matrix of a composite function</a></h4><time datetime="2019-12-23T17:58">December 23, 2019</time></figcaption></figure><figure><figcaption><h4><a href="https://sdamadi.github.io/what-is-jacobian-and-why-do-we-need-it.html" class="inverse">What is Jacobian matrix and why do we need it?</a></h4><time datetime="2019-12-23T19:02">December 23, 2019</time></figcaption></figure><figure><figcaption><h4><a href="https://sdamadi.github.io/how-to-use-linear-nn-to.html" class="inverse">Linear neural network to creat $f(x)&#x3D; W^{\top}x$</a></h4><time datetime="2019-12-25T17:57">December 25, 2019</time></figcaption></figure></div></div></footer></article></main><footer class="footer"><div class="footer__copyright">Powered by Publii</div></footer></div><script defer="defer" src="https://sdamadi.github.io/assets/js/jquery-3.2.1.slim.min.js?v=f307ecc7f6353949f03c06ffa70652a2"></script><script defer="defer" src="https://sdamadi.github.io/assets/js/scripts.min.js?v=f503b0ac46db9f99180b3b9f932c7f9a"></script></body></html>